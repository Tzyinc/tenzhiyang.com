<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><style id="typography.js">html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}progress{vertical-align:baseline}[hidden],template{display:none}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit;font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,input{overflow:visible}button,select{text-transform:none}[type=reset],[type=submit],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}html{font:100%/1.45 'georgia',serif;box-sizing:border-box;overflow-y:scroll;}*{box-sizing:inherit;}*:before{box-sizing:inherit;}*:after{box-sizing:inherit;}body{color:hsla(0,0%,0%,0.8);font-family:'georgia',serif;font-weight:normal;word-wrap:break-word;font-kerning:normal;-moz-font-feature-settings:"kern", "liga", "clig", "calt";-ms-font-feature-settings:"kern", "liga", "clig", "calt";-webkit-font-feature-settings:"kern", "liga", "clig", "calt";font-feature-settings:"kern", "liga", "clig", "calt";}img{max-width:100%;margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}h1{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:2rem;line-height:1.1;}h2{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:1.51572rem;line-height:1.1;}h3{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:1.31951rem;line-height:1.1;}h4{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:1rem;line-height:1.1;}h5{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:0.87055rem;line-height:1.1;}h6{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;color:inherit;font-family:-apple-system,'BlinkMacSystemFont','Segoe UI','Roboto','Oxygen','Ubuntu','Cantarell','Fira Sans','Droid Sans','Helvetica Neue',sans-serif;font-weight:bold;text-rendering:optimizeLegibility;font-size:0.81225rem;line-height:1.1;}hgroup{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}ul{margin-left:1.45rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;list-style-position:outside;list-style-image:none;}ol{margin-left:1.45rem;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;list-style-position:outside;list-style-image:none;}dl{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}dd{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}p{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}figure{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}pre{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;font-size:0.85rem;line-height:1.45rem;}table{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;font-size:1rem;line-height:1.45rem;border-collapse:collapse;width:100%;}fieldset{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}blockquote{margin-left:1.45rem;margin-right:1.45rem;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}form{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}noscript{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}iframe{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}hr{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:calc(1.45rem - 1px);background:hsla(0,0%,0%,0.2);border:none;height:1px;}address{margin-left:0;margin-right:0;margin-top:0;padding-bottom:0;padding-left:0;padding-right:0;padding-top:0;margin-bottom:1.45rem;}b{font-weight:bold;}strong{font-weight:bold;}dt{font-weight:bold;}th{font-weight:bold;}li{margin-bottom:calc(1.45rem / 2);}ol li{padding-left:0;}ul li{padding-left:0;}li > ol{margin-left:1.45rem;margin-bottom:calc(1.45rem / 2);margin-top:calc(1.45rem / 2);}li > ul{margin-left:1.45rem;margin-bottom:calc(1.45rem / 2);margin-top:calc(1.45rem / 2);}blockquote *:last-child{margin-bottom:0;}li *:last-child{margin-bottom:0;}p *:last-child{margin-bottom:0;}li > p{margin-bottom:calc(1.45rem / 2);}code{font-size:0.85rem;line-height:1.45rem;}kbd{font-size:0.85rem;line-height:1.45rem;}samp{font-size:0.85rem;line-height:1.45rem;}abbr{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}acronym{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;}abbr[title]{border-bottom:1px dotted hsla(0,0%,0%,0.5);cursor:help;text-decoration:none;}thead{text-align:left;}td,th{text-align:left;border-bottom:1px solid hsla(0,0%,0%,0.12);font-feature-settings:"tnum";-moz-font-feature-settings:"tnum";-ms-font-feature-settings:"tnum";-webkit-font-feature-settings:"tnum";padding-left:0.96667rem;padding-right:0.96667rem;padding-top:0.725rem;padding-bottom:calc(0.725rem - 1px);}th:first-child,td:first-child{padding-left:0;}th:last-child,td:last-child{padding-right:0;}</style><style data-href="/styles.3b1ac24b7f478a2fbbb1.css">:root{--text:#1d3557;--bg:#f1faee;--err:#e63946;--alt:#457b9d;--conf:#a8dadc}html{background-color:var(--bg);color:var(--text)}a{color:var(--alt)}.body{color:var(--text)}.parallax{background-attachment:fixed;background-position:50%;background-repeat:no-repeat;background-size:cover}</style><meta name="generator" content="Gatsby 2.13.28"/><style type="text/css">.gatsby-resp-image-image{width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;color:transparent;}</style><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><link rel="alternate" type="application/rss+xml" href="/rss.xml"/><link rel="shortcut icon" href="/icons/icon-48x48.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="manifest" href="/manifest.webmanifest"/><meta name="theme-color" content="#1D3557"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=a7842f193bac8b5e6875a60632aa12cb"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=a7842f193bac8b5e6875a60632aa12cb"/><title data-react-helmet="true">AI for FE devs (Part 2) | Ten&#x27;s Thoughts</title><meta data-react-helmet="true" name="description" content="Part 1 here This article is a complement to my presentation in React Knowledgable. Now that we know how to approach problems, we can look at what’s available…"/><meta data-react-helmet="true" property="og:title" content="AI for FE devs (Part 2)"/><meta data-react-helmet="true" property="og:description" content="Part 1 here This article is a complement to my presentation in React Knowledgable. Now that we know how to approach problems, we can look at what’s available…"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" name="twitter:creator" content="Ten Zhi Yang"/><meta data-react-helmet="true" name="twitter:title" content="AI for FE devs (Part 2)"/><meta data-react-helmet="true" name="twitter:description" content="Part 1 here This article is a complement to my presentation in React Knowledgable. Now that we know how to approach problems, we can look at what’s available…"/><link as="script" rel="preload" href="/webpack-runtime-6bfc57ca0b093ff56bf5.js"/><link as="script" rel="preload" href="/styles-18e3882dde0fcd7e69e2.js"/><link as="script" rel="preload" href="/app-bdfdfe73c40e7f941552.js"/><link as="script" rel="preload" href="/0-54b29570ea2c015b9713.js"/><link as="script" rel="preload" href="/1-5766929d39e4b70f596e.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-3255c5b6bdc52e892ed9.js"/><link as="fetch" rel="preload" href="/page-data/2019-08-01-ai-for-fe-2/page-data.json" crossorigin="use-credentials"/></head><body class="body"><noscript id="gatsby-noscript">This app works best with JavaScript enabled.</noscript><div id="___gatsby"><div style="outline:none" tabindex="-1" role="group" id="gatsby-focus-wrapper"><div style="margin-left:auto;margin-right:auto;max-width:34.8rem;padding:2.175rem 1.0875rem"><header><h3 style="margin-top:0"><a style="box-shadow:none;text-decoration:none;color:inherit" href="/">Ten&#x27;s Thoughts</a></h3></header><main><h1 style="margin-top:1.45rem;margin-bottom:0">AI for FE devs (Part 2)</h1><p style="font-size:0.87055rem;line-height:1.45rem;display:block;margin-bottom:1.45rem">August 01, 2019</p><div><h2><a href="https://blog.tenzhiyang.com/2019-07-31-ai-for-fe/">Part 1 here</a></h2>
<p><strong>This article is a complement to my presentation in React Knowledgable.</strong></p>
<p>Now that we know how to approach problems, we can look at what’s available. For this section I want to go through some big idea explanations of algorithms and what kind of input/output you expect from and implementation from the web. The goal here is that when you see a problem that you want to solve with AI, you have a rough idea what to google,and how to transform your data to pass into these algorithms.</p>
<h2>Fantastic algorithms and how to use them</h2>
<h3>Markov Process (Markov Models)</h3>
<p>The previous part made use of a <code class="language-text">Markov Chain</code>. The main idea for how Markov Processes work is that the future state is based on the current state and not anything more. We can define what is the “current” state like how we did with the text generation, where we defined the “current state” most recent two words to predict the next word. Markov Models are great if you have lots of data with relatively few “states”. So how do we define what is a ‘state’? Basically the very thing that you want to be predicted from a Markov model. Since the english language has 171,476 unique words (according to a google search), it is a relatively small number of states. This is why markov models are used very often in text prediction, such as those in your android and iphone keyboards.</p>
<p>Implementation details may vary, but the general idea taking a series of <code class="language-text">A -&gt; B</code> (A leads to B, or mathematically, A implies B) and passing it into a Markov Process. If you are dealing with something like a source text, where the states can be very long strings like <code class="language-text">A -&gt; B -&gt; C -&gt; ... -&gt; Z</code> you can just pass the whole data into some Markov chain library. Another example that isn’t text is something like weather prediction. If it has been raining today, it will be less likely to rain tomorrow. So you can just pass the weather over the past 10 years or so in the form of <code class="language-text">sunny, cloudy, rain, rain, sunny, ..., sunny</code> into the Markov process. Then after the data transformation is done, given the current weather, you’ll get a prediction of what’s the next weather. </p>
<p>As you might observe, real life is not so obvious that you can predict X given Y and get a correct prediction all the time. Hence markov processes are very useful for problems that give you a list of what’s the probablity of things happening after Y has happened, and a user makes an informed decision. Alternatively it can also be used for things where randomness is a bonus side effect, such as in the text generator, where I just chose a random word from the list of X predicted from Ys.</p>
<h3>K Nearest Neighbours (KNN)</h3>
<p>This algorithm is really a very simple one. Going back to our 2-dimensional space of data, we plot all these points on the graph, and then we add a label to each one. There’s no calculations being done during the “training” time, really it’s just data entry. So when we actually need to classify a new point, what we do is that we iterate through all the data, and then we find the K (some constant that you will input) nearest neighbours, using a simple straight-line (Euclidean) distance calculation. To use a KNN library, you should really only need to pass in the parameter and the label. Just note that some libraries do not accept negative parameter values.</p>
<p>The great thing about KNN is that adding data really doesn’t cost anything at all, as there’s no computation. KNN scales linearly with large number of data, and large number of parameters, and this calculation is only done during the categorising process.</p>
<h3>Random Forest Classification</h3>
<p>Random forests comes as a variant of <code class="language-text">decision trees</code>. A decision tree is built using some source dataset, split based on some rules based on the parameters. Each split that we make creates a “branch” with some probabilites on each branch. Eventually all branches have smaller branches until we form what looks like an upside down tree: </p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/d3e1067cf318eb8febb657cad57be757/a296c/dectree.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuxQbAx1CQf//EABsQAAEEAwAAAAAAAAAAAAAAAAEAAhEhEDFB/9oACAEBAAEFAnaFFONzODa4v//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABcQAAMBAAAAAAAAAAAAAAAAAAAQEQL/2gAIAQEABj8CI6tP/8QAHBAAAQQDAQAAAAAAAAAAAAAAAQAQETEhQWHB/9oACAEBAAE/ISjpFIRbwqzbDBLmkANG/9oADAMBAAIAAwAAABBzyDz/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAeEAEAAgICAwEAAAAAAAAAAAABESEAMRBhUZGh0f/aAAgBAQABPxCmGFR73iSlIJ4OuJEOil9K/QxEEoIJ7v8AMGSTWWFSAQqKmcjBDITW+P/Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="decision tree"
        title="decision tree"
        src="/static/d3e1067cf318eb8febb657cad57be757/c739e/dectree.jpg"
        srcset="/static/d3e1067cf318eb8febb657cad57be757/8ee9c/dectree.jpg 148w,
/static/d3e1067cf318eb8febb657cad57be757/ebbe7/dectree.jpg 295w,
/static/d3e1067cf318eb8febb657cad57be757/c739e/dectree.jpg 590w,
/static/d3e1067cf318eb8febb657cad57be757/a296c/dectree.jpg 800w"
        sizes="(max-width: 590px) 100vw, 590px"
      />
  </a>
    </span></p>
<p>A random forest is just the same process, but splitting the data into random subsets of parameters, and creating a separate tree for each of them, hence the term random forest classification. The algorithm will then return a list of different classifications with some number from 0-1, this number is known as the <code class="language-text">confidence</code> for the purpose of this blog post, you can think of confidence as exactly what it sounds like. How confident the algorithm is that this is the classification.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/6692c4276cc9790d3bf0b66d620f0c3b/a296c/randomforest.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe/ki0C8Cgf/xAAYEAACAwAAAAAAAAAAAAAAAAAAEAEhQv/aAAgBAQABBQJ6LUL/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAcEAABBAMBAAAAAAAAAAAAAAABABARIUFRgbH/2gAIAQEAAT8hlSblrQ50gAFR1g9b/9oADAMBAAIAAwAAABAjyAD/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAdEAEAAgICAwAAAAAAAAAAAAABABEhMUFhEFFx/9oACAEBAAE/EGjW14gGhpzRqDZiU1oDJ8hQYPB7zCoKekxBMwMrblt78f/Z'); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="random forest"
        title="random forest"
        src="/static/6692c4276cc9790d3bf0b66d620f0c3b/c739e/randomforest.jpg"
        srcset="/static/6692c4276cc9790d3bf0b66d620f0c3b/8ee9c/randomforest.jpg 148w,
/static/6692c4276cc9790d3bf0b66d620f0c3b/ebbe7/randomforest.jpg 295w,
/static/6692c4276cc9790d3bf0b66d620f0c3b/c739e/randomforest.jpg 590w,
/static/6692c4276cc9790d3bf0b66d620f0c3b/a296c/randomforest.jpg 800w"
        sizes="(max-width: 590px) 100vw, 590px"
      />
  </a>
    </span></p>
<p>One great thing about Random forest classification is since you have a list of results of different confidences, if you have non mutually exclusive groups to classify into (eg: it can be <strong>both</strong> raining and sunny), you can actually choose to accept results above some level of confidence. Random forest algorithms normally take in an array of parameters and an array of output. The parameters need not be the same length, but parameter order matters if you want fairly accurate (greater than 90 percent) results. The output order doesn’t matter for most implementations.</p>
<h3>Neural Networks</h3>
<p>The absolute coolest kid on the block, nerual networks has already been explained in a previous talk, but the general idea is that we are trying to simulate how the brain works. Starting from a neuron, each neuron takes in a large number of inputs, multiply each input by some number (called weights, which define how important each input is), sums these inputs together, runs a type of mathematical function (called activation function) such that the result is between 0-1. the output of this neuron is then passed to another neuron, and so on and so forth. Each group of neurons that passes on to the next group of neurons is known as a “layer”. See how similar a NN neuron is to a brain neuron:</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/ac83c7e7d2ce969673ce8b48fb237ca1/a296c/neuron.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe7UNoJQA//EABYQAQEBAAAAAAAAAAAAAAAAACARIf/aAAgBAQABBQLRT//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABQQAQAAAAAAAAAAAAAAAAAAADD/2gAIAQEABj8CH//EABkQAQADAQEAAAAAAAAAAAAAAAEQESEAMf/aAAgBAQABPyF1nF7bC0958YS5/9oADAMBAAIAAwAAABBQxzz/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/EB//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/EB//xAAdEAEAAgICAwAAAAAAAAAAAAABABEhMRBBUWGB/9oACAEBAAE/ECr2A97huiR0BrjpF+Q2dHiGoCzbmUVVY4//2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="brain vs nn"
        title="brain vs nn"
        src="/static/ac83c7e7d2ce969673ce8b48fb237ca1/c739e/neuron.jpg"
        srcset="/static/ac83c7e7d2ce969673ce8b48fb237ca1/8ee9c/neuron.jpg 148w,
/static/ac83c7e7d2ce969673ce8b48fb237ca1/ebbe7/neuron.jpg 295w,
/static/ac83c7e7d2ce969673ce8b48fb237ca1/c739e/neuron.jpg 590w,
/static/ac83c7e7d2ce969673ce8b48fb237ca1/a296c/neuron.jpg 800w"
        sizes="(max-width: 590px) 100vw, 590px"
      />
  </a>
    </span></p>
<p>The training algorithm will take in some input and pass it through the neural network, with all the weights randomised, and gets a prediction, which can be one or more numbers, depending on how many labels you want. It will then use the correct label and compare with the current label and tweak the weights such that the next time we pass the input, we are more likely to get a result closer to the correct label. We don’t alter the weights such that it definitely gives us the correct label because we do not want the neural net work to only work on the training set, we want it to work in real life scenarios with ambiguous input that is similar but not exactly the same with the dataset.</p>
<p>I don’t recommend training your own neural network when you’re just beginning as you need a huge amount of data (at least 500 samples of each label) and creating a neural network is really more of an art than a science. It takes experience and some intuition to really know how many layers and how big a layer you want for a neural network. However, there’s proably a pre-trained model that solves for your problem, and using it should be easy enough, just that every model has different requirements. You’ll need to read their respective documentation to use them.</p>
<h3>Convolutional Neural Networks</h3>
<p>The idea of convolutional nerual networks is simple in implementation but not very intuitive to understand. Basically for an image, Each pixel is a parameter (or rather, 3 or 4, with Red Green Blue and transparency values). You can <strong>technically</strong> pass in the entire image into a Neural network, but it usually doesnt work out well, as a normal neural network does not understand the relations between pixels. Another problem is that having each pixel as a parameter is too large an input and would take forever to train.</p>
<p>This is where the convolutional layer and pooling layer comes in. Basically the convolutional layer applies a filter, which transforms <code class="language-text">n * n</code>pixels (a square) into 1, using some matrix manipulation mathemagic. The pooling layer comes immediately after the convolutional layer, and its job is to reduce the input space further, by taking the max or the average value in <code class="language-text">n * n</code> pixels. These are called <code class="language-text">Max Pooling</code> and <code class="language-text">Average Pooling</code>. Max pooling by nature tends to reduce noise in the data while Average pooling is a simple <code class="language-text">dimension reduction</code> (reducing the input size by <code class="language-text">x</code> dimensions). The two layers together will be inserted in between some layers of a nerual network. It is possible to have more than one such convolution+pooling layer, but that increases computation time.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/073155acbd5405378a4882395a963ac7/a296c/cnn.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAIDAAAAAAAAAAAAAAAAAAECAwQF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAe8pijaQpULg/8QAGRABAAIDAAAAAAAAAAAAAAAAAQIQACEx/9oACAEBAAEFAsGpcjpoK//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABQQAQAAAAAAAAAAAAAAAAAAADD/2gAIAQEABj8CH//EABoQAAIDAQEAAAAAAAAAAAAAAAERABBRITH/2gAIAQEAAT8hJKwxgrtDG0FewAdf/9oADAMBAAIAAwAAABAkzwD/xAAVEQEBAAAAAAAAAAAAAAAAAAAgIf/aAAgBAwEBPxCD/8QAFhEBAQEAAAAAAAAAAAAAAAAAEQEg/9oACAECAQE/EGpj/8QAGxABAAIDAQEAAAAAAAAAAAAAARExABBBIVH/2gAIAQEAAT8Qk4PgvphFKL3RtPemQlpRpAQkmOoue6//2Q=='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="convolution and pooling layer"
        title="convolution and pooling layer"
        src="/static/073155acbd5405378a4882395a963ac7/c739e/cnn.jpg"
        srcset="/static/073155acbd5405378a4882395a963ac7/8ee9c/cnn.jpg 148w,
/static/073155acbd5405378a4882395a963ac7/ebbe7/cnn.jpg 295w,
/static/073155acbd5405378a4882395a963ac7/c739e/cnn.jpg 590w,
/static/073155acbd5405378a4882395a963ac7/a296c/cnn.jpg 800w"
        sizes="(max-width: 590px) 100vw, 590px"
      />
  </a>
    </span></p>
<p>There are plenty of pre-trained convolutional neural networks that recognises objects, animals, drawings or even individual hand written characters, so if you just need to identify what is inside an image, you can just leverage on the existing neural nets to classify what you need. For example, during the most recent hackathon I wanted to know if the user has drawn a circle, square or diamond, my team used google quickdraw’s net to identify what it was and re-draw a perfect circle square or diamond in its place. I did some additional neural net on top of that, which causes the lag, but you can try it <a href="https://hackathon.flowdi.tenzhiyang.com/">here</a>.</p>
<h3>Recurrent Neural network</h3>
<p>As you might notice from the Neural networks and CNN sections, the way we simulate the human brain may be a bit simplified, and RNN aims to improve on one aspect. Recurrent Neural Networks aim to add a “short term memory” by making each neuron pass its output back to itself, for the next computation. This allows information to persist inside the network where the previously input data will affect the next few predictions. There is a commonly used variant of RNN called Long Short-Term Memory (<code class="language-text">LTSM</code>) network which works differently but produces better results.</p>
<p><span
      class="gatsby-resp-image-wrapper"
      style="position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;"
    >
      <a
    class="gatsby-resp-image-link"
    href="/static/eccf8bf44a10fd21843ce203fd146772/a296c/recc.jpg"
    style="display: block"
    target="_blank"
    rel="noopener"
  >
    <span
    class="gatsby-resp-image-background-image"
    style="padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe/ENggKD//EABcQAQADAAAAAAAAAAAAAAAAAAEAECD/2gAIAQEAAQUChZn/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAECAQE/AR//xAAUEAEAAAAAAAAAAAAAAAAAAAAw/9oACAEBAAY/Ah//xAAZEAADAQEBAAAAAAAAAAAAAAABEBEAIUH/2gAIAQEAAT8h1dqIu9sCL//aAAwDAQACAAMAAAAQ8AgA/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPxAf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPxAf/8QAGxAAAQUBAQAAAAAAAAAAAAAAAQAQETFBIVH/2gAIAQEAAT8QWhvGGInDIQQAuDfrgoG//9k='); background-size: cover; display: block;"
  ></span>
  <img
        class="gatsby-resp-image-image"
        alt="Recurrent Neural Network"
        title="Recurrent Neural Network"
        src="/static/eccf8bf44a10fd21843ce203fd146772/c739e/recc.jpg"
        srcset="/static/eccf8bf44a10fd21843ce203fd146772/8ee9c/recc.jpg 148w,
/static/eccf8bf44a10fd21843ce203fd146772/ebbe7/recc.jpg 295w,
/static/eccf8bf44a10fd21843ce203fd146772/c739e/recc.jpg 590w,
/static/eccf8bf44a10fd21843ce203fd146772/a296c/recc.jpg 800w"
        sizes="(max-width: 590px) 100vw, 590px"
      />
  </a>
    </span></p>
<p>The nature of having some form of short term memory makes RNN especially useful for things like speech recognition, language modeling, image captioning or even some time-based data (known as <code class="language-text">time-series</code>) such as stock price of a company. I haven’t really used RNN yet, but it’s good to know what its capable of.</p>
<h3>Transfer Learning</h3>
<p>Earlier I mentioned about not training your own neural network because it (probably) already exists, but here’s something we can do to make a neural network solve more domain specific problems. Transfer learning is basically using an existing neural net, and training the top (few) layers of the neural net, so that you retain the pattern-recognition capabilities of your neural network, but optimised to your specific problem. There are plenty of tutorials and methods differ between various libraries, so I won’t go into the details of implementation.</p>
<p>However, one interesting take on transfer learning was something I learnt from the <a href="https://www.tensorflow.org/js/tutorials/transfer/image_classification">tensorflowjs transfer learning tutorial</a>. Essentially what this method does is that it takes a representation (they call it an <code class="language-text">activation</code>) of the neural network, and uses it as the parameter of a K-Nearest Neighbour. I asked some data scientist friends of mine and they have confirmed that this is not really a traditional implementation of transfer learning, although it’s a vert interesting idea.</p>
<p>The idea behind this concept is perhaps best explained in an example. <a href="https://httpserve.tenzhiyang.com/imageRecognition/">This Project</a> uses image net, which is trained to recognise object from images. Even if the net is not trained to recognise something, for example which direction my head is facing, when the image changes drastically, there are some neurons in the network that will have a high value and some neurons that wont change at all. Therefore knowing which are the “triggered” and “non-triggered” neurons as parameters, we are able to categorise new objects using KNN, using a model that learnt from another data set.</p>
<h2>Example Projects</h2>
<p>Here are a few projects that I’ve been doing in the past couple of weeks, where each project takes about a couple of hours to build, I do try to add something on top of just consuming the api, so that you can catch a glimpse of how I make use of existing tools to do something else instead of following tutorials or documentation to solve a problem that someone has already solved before.</p>
<p>All of these projects are running entirely on the front end, they are all static sites served off a simple http server.</p>
<h3><a href="https://httpserve.tenzhiyang.com/gumshoos/">Gumshoos</a></h3>
<ul>
<li>Time taken: 45 mins, of which maybe 30 was looking for a super simple no frills implementation of markov chains.</li>
<li>Libraries: <code class="language-text">nlp_compromise</code></li>
</ul>
<h3><a href="https://httpserve.tenzhiyang.com/imageRecognition/">Image Recognition</a></h3>
<ul>
<li>Time taken: 1.5 hours, half of the time spent following the google codelab tutorial</li>
<li>Libraries/models: <code class="language-text">tensorflowjs</code> <code class="language-text">mobilenet</code> <code class="language-text">knn-classifier</code></li>
</ul>
<h3><a href="https://httpserve.tenzhiyang.com/greenScreen/">Green Screen</a></h3>
<ul>
<li>Time taken: 1.5 hours</li>
<li>Libraries/models: <code class="language-text">tensorflowjs</code> <code class="language-text">body-pix</code> <code class="language-text">jscolor</code></li>
</ul>
<p>Body-pix is a model that is used for segmenting body parts in an image. Here I made use of a function <code class="language-text">toMaskImageData</code> which basically gives me a “silhouette” of a person in the image. I use a few composite methods available from the canvas context api (webcam input, overlay silhouette with <code class="language-text">destination-in</code>) to remove the background. Then green screen in the background is really just setting a background color. Theoretically we can overlay the greenscreen effect over anything, as long as they’re both in the same webpage.</p>
<h3><a href="https://httpserve.tenzhiyang.com/mocap/">Mocap</a></h3>
<ul>
<li>Time taken: 1 hour</li>
<li>Libraries/models: <code class="language-text">tensorflowjs</code> <code class="language-text">posenet</code></li>
</ul>
<p>I really didn’t do anything for this. Passing in the webcam-feed into <code class="language-text">posenet</code> I receive a list of bodyparts followed by their x and y coordinates and a confidence score. I then drew red squares where each body part was. I can go a step further to remove those body parts with low confidence, but that’s just from analysing the input and output and using some constant value that works well enough.</p>
<h3><a href="https://httpserve.tenzhiyang.com/jojoPose/">Jojo’s Bizarre Poses</a></h3>
<ul>
<li>Time taken: 1 hour (excluding the time required to build mocap)</li>
<li>Libraries/models: <code class="language-text">tensorflowjs</code> <code class="language-text">posenet</code> <code class="language-text">knn-classifier</code></li>
</ul>
<p>This is like a combination of <code class="language-text">mocap</code> and <code class="language-text">image recognition</code> projects above, but since the KNN classifier I used was from the tensorflow libraries, it takes in a certain data type <code class="language-text">tensors</code> as input. I had to modify the data to fit the KNN classifier.</p>
<h2>Some useful tools</h2>
<p>What I went through in the past two blog posts cannot possibly cover all the use cases you might want to work on, so I’ve decided to compile a list of useful libraries and data sets that you can use in your own personal projects.</p>
<h3>Some famous Datasets</h3>
<ul>
<li>Brown Corpus</li>
</ul>
<p>Known formally as the <strong>Brown University Standard Corpus of Present-Day American English</strong> this dataset contains about 500 samples of english-language text and is used frequently in NLP projects.</p>
<ul>
<li>Quick draw</li>
</ul>
<p>Remember <a href="https://quickdraw.withgoogle.com/?locale=en_US">this game</a>? It was all a data collection exercise, and they’ve open sourced all the drawings together with the draw order!</p>
<ul>
<li>ImageNet</li>
</ul>
<p>A database of pictures that are labeled by crowd-sourcing, this dataset contains more than 20,000 categories, with bounding boxes to highlight the objects being labeled.</p>
<ul>
<li>MS-COCO</li>
</ul>
<p><strong>Common Objects in COntext</strong>, this is a dataset of common objects are taken from every day scenes. Instead of bounding boxes like in ImageNet, they have object segmentation, just like my green-screen example, but for objects.</p>
<ul>
<li>MNIST</li>
</ul>
<p>MNIST database (<strong>Modified National Institute of Standards and Technology</strong> database) Is a database of handwritten digits and is used very frequently to practice creating handwritten character recognition algorithms</p>
<ul>
<li>Free Spoken Digit</li>
</ul>
<p>This is the MNIST of Speech recognition basically.</p>
<ul>
<li>Free Music Archive</li>
</ul>
<p>Just an archive of royalty free music!</p>
<h3>Some famous Libraries</h3>
<ul>
<li>ML.js</li>
</ul>
<p>The scikit AI toolset, now replicated in js, has lots of learning algorithms and some useful data transforming utilities.</p>
<ul>
<li>compromise</li>
</ul>
<p>A fast NLP library which helps format text. Uses some outdated techniques in favour of speed over accuracy, aims to solve 90% of usecases.</p>
<ul>
<li>Tensorflowjs</li>
</ul>
<p>A re-write of the famous tensorflow in js. There’s quite an established community supporting and I can find very good models written for or compatible with tensorflowjs</p>
<ul>
<li>Kerasjs</li>
</ul>
<p>A fairly popular neural network library, the models produced by keras can be imported into tensorflowjs also.</p>
<ul>
<li>ML5</li>
</ul>
<p>Another Nerual Network library. It is actually a high-level interfact for tensorflowjs. I’ve heard that ML 5 has more beginner friendly jargon and documentation, but I don’t have experience with it yet.</p></div><hr style="margin-bottom:1.45rem"/><div style="display:flex;margin-bottom:3.625rem"><div class=" gatsby-image-wrapper" style="position:relative;overflow:hidden;display:inline-block;width:50px;height:50px;margin-right:0.725rem;margin-bottom:0;min-width:50px;min-height:50px;border-radius:100%"><img src="data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGgABAAIDAQAAAAAAAAAAAAAAAAQFAQIGA//EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGbimmy2LnSdNqV6g//xAAbEAACAwADAAAAAAAAAAAAAAABAgADERMhIv/aAAgBAQABBQJr1xbSJz1xPL2Da8iooVB2UQn/xAAWEQEBAQAAAAAAAAAAAAAAAAARABD/2gAIAQMBAT8BI3//xAAWEQEBAQAAAAAAAAAAAAAAAAARABD/2gAIAQIBAT8BZ3//xAAfEAACAQIHAAAAAAAAAAAAAAAAAiEBMRARIiMycYH/2gAIAQEABj8C0SxuVXw5FjJZwilxeiVof//EAB8QAQADAAEEAwAAAAAAAAAAAAEAESExEEFRYXGB8P/aAAgBAQABPyHnQjQM7bHhn8CWF16jVDrc6c+418wLwC059S9Lfif/2gAMAwEAAgADAAAAECT3PP/EABYRAAMAAAAAAAAAAAAAAAAAAAEgIf/aAAgBAwEBPxASj//EABgRAAIDAAAAAAAAAAAAAAAAAAABEBEh/9oACAECAQE/EHopP//EAB0QAQEAAgIDAQAAAAAAAAAAAAERACFBUTFh0ZH/2gAIAQEAAT8QNBqEQryvRjiL5Gx6TPT/AF8wCqjGK5aETINBvUxQ+MVQNJbo3XAvRRE3PrLDLlNz/9k=" alt="Ten Zhi Yang" style="position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;object-position:center;opacity:1;transition-delay:500ms;border-radius:50%"/><noscript><picture><source srcset="/static/b4478bd21385e928d199d80538c411b8/9b664/profile-pic.jpg 1x,
/static/b4478bd21385e928d199d80538c411b8/06a10/profile-pic.jpg 1.5x,
/static/b4478bd21385e928d199d80538c411b8/f1b5a/profile-pic.jpg 2x" /><img loading="lazy" width="50" height="50" srcset="/static/b4478bd21385e928d199d80538c411b8/9b664/profile-pic.jpg 1x,
/static/b4478bd21385e928d199d80538c411b8/06a10/profile-pic.jpg 1.5x,
/static/b4478bd21385e928d199d80538c411b8/f1b5a/profile-pic.jpg 2x" src="/static/b4478bd21385e928d199d80538c411b8/9b664/profile-pic.jpg" alt="Ten Zhi Yang" style="position:absolute;top:0;left:0;opacity:1;width:100%;height:100%;object-fit:cover;object-position:center"/></picture></noscript></div><div><div><a href="https://twitter.com/tzyinc">Ten Zhi Yang</a> is a dev in Singapore. He enjoys playing with new technology</div></div></div><hr style="margin-bottom:1.45rem"/><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li><a rel="prev" href="/2019-07-31-ai-for-fe/">← <!-- -->AI for FE devs (Part 1)</a></li><li></li></ul></main><footer><a href="https://twitter.com/tzyinc">Twitter</a> • <a href="https://drive.google.com/open?id=1iVoWXxYpAlL2QG3O9bFFXqX6lJw-JbdcX9gfYw7gIzU">CV</a></footer></div></div></div><script>
  
  
  if(true) {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  }
  if (typeof ga === "function") {
    ga('create', 'undefined', 'auto', {});
      
      
      
      }
      </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2019-08-01-ai-for-fe-2/";window.webpackCompilationHash="225a25dac73efe655bbc";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-bdfdfe73c40e7f941552.js"],"component---node-modules-gatsby-plugin-offline-app-shell-js":["/component---node-modules-gatsby-plugin-offline-app-shell-js-fe7880bfc69bb16a9d17.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-3255c5b6bdc52e892ed9.js"],"component---src-pages-404-js":["/component---src-pages-404-js-52918f1847c8f844b974.js"],"component---src-pages-index-js":["/component---src-pages-index-js-3e532f44c568bf19043e.js"]};/*]]>*/</script><script src="/component---src-templates-blog-post-js-3255c5b6bdc52e892ed9.js" async=""></script><script src="/1-5766929d39e4b70f596e.js" async=""></script><script src="/0-54b29570ea2c015b9713.js" async=""></script><script src="/app-bdfdfe73c40e7f941552.js" async=""></script><script src="/styles-18e3882dde0fcd7e69e2.js" async=""></script><script src="/webpack-runtime-6bfc57ca0b093ff56bf5.js" async=""></script></body></html>