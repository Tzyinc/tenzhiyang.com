{"componentChunkName":"component---src-templates-blog-post-js","path":"/2020-09-30-music-in-browser-basics/","result":{"data":{"site":{"siteMetadata":{"title":"Ten's Thoughts","author":"Ten Zhi Yang"}},"markdownRemark":{"id":"f9b7ebb9-7da8-5f3c-a916-10fe7f99169c","excerpt":"I’ve recently started a tech in music interest group within Shopee that will run for 8 weeks, and I want to have some content out for the first week as we will…","html":"<p>I’ve recently started a tech in music interest group within Shopee that will run for 8 weeks, and I want to have some content out for the first week as we will be doing mostly introductions. So this shall be my welcome and beginner starting kit to my 3 amigos Adriel, Stanley and James.</p>\n<p>This post will try to provide an non technical explanation for some of the magic behind the web audio api so that we know what to google and research on our own. <strong>This article is suitable for people comfortable with working in JS, with minimal to no signal processing background knowledge</strong></p>\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Basic_concepts_behind_Web_Audio_API\">reference</a></p>\n<h1>Web audio api</h1>\n<p>The web audio api is powerful in that you should be able to build anything you need to relating sound, but as I found out, it’s not very easy to use without some background in signal processing. All of our operations will revolve around the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AudioContext\">Audio Context</a>, which is essentially a graph built from modular <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AudioNode\">Audio Node</a>s One audio context can have multiple Audio nodes. An audio node can be an audio source, an audio destination or some audio processing module. These nodes are routed to each other (eg, source -> processing -> destination) and form a graph. There can be multiple input and output nodes.</p>\n<h3>Audio sampling</h3>\n<p>Because it’s not very feasible to store a continuous audio input, we usually <code class=\"language-text\">sample</code> <a href=\"https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time\">the continuous signal into a discrete representation</a>, in the form of X values at Y time</p>\n<h2>Audio Sources</h2>\n<p>After creating an audio context, we usually want to take an input, known as an audio source. Audio sources can be in the form of:</p>\n<ul>\n<li>sound generated from js (eg. creating a signal via <code class=\"language-text\">oscillator</code>)</li>\n<li>raw audio data known as <a href=\"https://en.wikipedia.org/wiki/Pulse-code_modulation\">PCM</a> (eg. <code class=\"language-text\">AudioBuffer</code>)</li>\n<li>HTML elements like <code class=\"language-text\">&lt;audio&gt;</code> or <code class=\"language-text\">&lt;video&gt;</code></li>\n<li>or some <code class=\"language-text\">MediaStream</code> like your microphone</li>\n</ul>\n<h2>Effects</h2>\n<p>A lot of cool interactions we want to do involves adding effects to audio sources. We usually use the term <code class=\"language-text\">connect</code> to apply these effects onto an audio context Some things we can do is:</p>\n<ul>\n<li><code class=\"language-text\">GainNode</code> change the volume. We can add gain to an input to increase the loudness </li>\n<li><code class=\"language-text\">ConvolverNode</code> gives reverb effects</li>\n<li><code class=\"language-text\">WaveShaper</code> applies a non-liner distorter, can be used to change the tone of the sound.</li>\n<li><code class=\"language-text\">DynamicsCompressorNode</code> lowers volume of loudest parts in the signal to help reduce clipping and distortion</li>\n</ul>\n<h2>Destination (Output)</h2>\n<p>We can use <code class=\"language-text\">AudioDestinationNode</code> to select the audio source, usually for speakers of the device. Alternatively we can have <code class=\"language-text\">MediaStreamAudioDestinationNode</code> it uses <code class=\"language-text\">WebRTC</code> <code class=\"language-text\">MediaStream</code> under the hood. An interesting thing we can do with audio output is to have <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics\">spatialization effects</a> to simulate where the sound is coming from.</p>\n<h2>Analysis</h2>\n<p>The <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode\">AnalyserNode</a> allows us to take numerical representations of the audio context. This is usually used for [visualizations] (<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API\">https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Visualizations_with_Web_Audio_API</a>)</p>","frontmatter":{"title":"Music in browser basics","date":"September 30, 2020"}}},"pageContext":{"slug":"/2020-09-30-music-in-browser-basics/","previous":{"fields":{"slug":"/2020-06-06-twatter-talk/"},"frontmatter":{"title":"A look into building twatter"}},"next":null}},"staticQueryHashes":["426816048","444714259"]}