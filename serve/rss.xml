<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Ten's Thoughts]]></title><description><![CDATA[Just another Dev blog]]></description><link>https://blog.tenzhiyang.com</link><generator>GatsbyJS</generator><lastBuildDate>Sat, 12 Oct 2019 17:17:44 GMT</lastBuildDate><item><title><![CDATA[Wi is a good router so hard to Fi]]></title><description><![CDATA[In the past five years or so, I’ve been using the netgear orbi at my home. It gave us incredible speeds in a mesh network, but it costs…]]></description><link>https://blog.tenzhiyang.com/2019-10-12-wi-is-a-good-router-so-hard-to-fi/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-10-12-wi-is-a-good-router-so-hard-to-fi/</guid><pubDate>Sat, 12 Oct 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;In the past five years or so, I’ve been using the netgear orbi at my home. It gave us incredible speeds in a mesh network, but it costs upwards of 600SGD for two units (one router and one satallite), and it just couldn’t provide the cover I needed, nor could it daisy chain. Every time there was a firmware update, I had to restore factory settings and set up everything all over again. Definitely not a great experience. When my setup started dropping wifi connections every day, I needed a replacement, with better bang for my buck.&lt;/p&gt;
&lt;p&gt;The solution seemed straight forward: research for a new consumer mesh setup, and buy one. I researched on many different brands and setups, and the one thing that came out consistently in all the reviews was: the orbi was the best product your money can buy (╯°□°）╯︵ ┻━┻&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.tomsguide.com/us/netgear-orbi,review-4263.html&quot;&gt;Every&lt;/a&gt; &lt;a href=&quot;https://www.lifewire.com/netgear-orbi-review-4589368&quot;&gt;one&lt;/a&gt; &lt;a href=&quot;https://www.expertreviews.co.uk/netgear/1405475/netgear-orbi-rbk50-review&quot;&gt;seemed&lt;/a&gt; &lt;a href=&quot;https://www.techspot.com/products/routers/netgear-rbk50-rbr50-orbi-ac3000-tri-band-wifi.153730/&quot;&gt;to&lt;/a&gt; &lt;a href=&quot;https://www.mbreviews.com/netgear-orbi-home-wifi-system-review/&quot;&gt;be&lt;/a&gt; &lt;a href=&quot;https://www.techradar.com/sg/reviews/netgear-orbi&quot;&gt;bought&lt;/a&gt; off by netgear and yes, I considered that I might have been the minority and received a faulty unit, but the &lt;a href=&quot;https://www.reddit.com/r/orbi/&quot;&gt;community seems to agree with me&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In the end I figured to just look for the cheapest mesh units money can buy. I found a chinese company called tenda, selling the m5 series with a set of &lt;a href=&quot;https://shopee.sg/Tenda-Nova-MW6-WiFi-Wireless-Router-and-Repeater-2.4G-5.0GHz-APP-Remote-Manage-i.41816358.2236310726&quot;&gt;3 units under 200SGD&lt;/a&gt; and read all the reviews for it. It seemed like it could output a maximum of 900Mbps which was sufficient for my needs, and there was little to no configurations for it. There’s only a max of 2 cat5 ports but that was sufficient for most of my house, and I could always buy a desktop switch for the living room. The reviews also promised a plug and play but warned of almost no configuration flexibility. At that time it was also on sale for about 150SGD for a set of 3, hell yea.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/a4c6952dacc63ac092a696b1334b1e89/ab422/tenda.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;I bought it and a desktop switch (also tenda, because it’s cheap and just in case of compatibility issues), it arrived a week later. It did not work when connected with my modem. To be fair to tenda, connecting each mesh unit was as simple as starting up their proprietary app, and selecting each unit as they popped up on the screen. I found that I could get it to work if I connected my old router via lan to the tenda m5. This setup gave me about 600-900 Mbps but I had two different wifi endpoints, one working, one dying. &lt;/p&gt;
&lt;p&gt;This was good enough for me until my orbi started dropping lan too. Time to look for a gigabit router. This time, I wanted something that was tried and tested in Singapore. I looked through many different routers that promised gigabit speeds, but reading user reviews, it seemed like most cheap routers under a hundred had over inflated numbers. The WiFi could technically handle gigabit speed, but they were just using cheap rj45 connectors. WiFi was not my issue as I could continue to use the tenda m5 as my wifi access points.&lt;/p&gt;
&lt;p&gt;I then started to look into enterprise solutions. I figured if the machine did not have WiFi, at least the money I spent on would be for the hardware. In came the Edge Router X.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/fac4e61f2f1cf1273452414965df19f3/48a11/erx.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;At the comparitively low price of 85SGD, this little beast is rumored to be on the same level as gaming routers costing three times its price. It is essentially a network switch but with router software, and with hardware offloading introduced about 2 years ago in the firmware, it could handle gigabit speeds. Certain things were supposedly not accessible by the browser interface, but I felt I am fairly familiar with the Command Line at this point in my development career, what batter time than now to enter the world of enterprise routers? The only thing left was to see if it worked when paired with my modem.&lt;/p&gt;
&lt;p&gt;To my dismay, few people in Singapore used the edge router x, in favor of the edge router lite, which is a router built from the ground up. Do not be fooled by tha naming though, the lite is almost twice the price of the x, and could handle higher speeds than the ER-X out of the box. Shoutout to &lt;a href=&quot;https://twitter.com/jacobtyq/status/1182166814861611009&quot;&gt;Jacob&lt;/a&gt; for letting me know his experience with the ER lite! However, I am a cheapskate. I bought the ER-X during shopee’s recent 10.10 sale.&lt;/p&gt;
&lt;p&gt;Like every other tech nerd, the first thing I did when using a new technology was to not read any documentation and use it. I connected my modem to the ER-X and connected my laptop to the ER-X, and typed in &lt;code class=&quot;language-text&quot;&gt;192.168.1.1&lt;/code&gt; into my browser. I was greeted by my good friend:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/a4841ffc5664bdc74657d2bee6683b4e/135ae/chromerex.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;After actually reading some tutorials, I realised I had to connect my computer to eth0, set my laptop to request for a static IP (no default dhcp) and open the setup wizard (for noobs like me) to set up some basic configs. I applied all the default settings and changed the password of the default user. As part of the wizard and default settings, I had to disconnect my laptop from eth0 and connect to any other port, then connect my modem to eth0. I had some rx and tx going on the webgui, but I was not getting any internet. Dang. After some trial and error and about an hour of research, I found the solution to all things in tech support: turning everything off and on again. The key thing that made it work was restarting the modem, allowing my router to request a new IP from my ISP. Everything was working with minimal setup.&lt;/p&gt;
&lt;p&gt;The last thing I had to do was to enable hardware offloading. I do not know what hardware offloading does. I have read all the tutorials and all the explanations and like all tech documentations, they went over my head, but all I know is: turn on hardware offloading, you lose QoS but gain gigabit speed. (Note: Edgerouter lite doesn’t need to turn on hardware offloading to achieve gigabit speed) Activating these were simple enough: enter the CLI, log in, type &lt;code class=&quot;language-text&quot;&gt;command&lt;/code&gt; followed by &lt;code class=&quot;language-text&quot;&gt;set system offload hwnat enable&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;set system offload ipsec enable&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;commit ; save&lt;/code&gt;. Reboot the router and check from the cli with &lt;code class=&quot;language-text&quot;&gt;show ubnt offload&lt;/code&gt;. You can also do the same by fiddling with the GUI, but where’s the fun in that.&lt;/p&gt;
&lt;p&gt;Finally everything is up. Connecting my mesh routers to this router was seamless, as well as my smart tv and streaming boxes. Setting up port forwarding for my raspberry pi was as straight forward as any commercial router too. I haven’t really managed to get gigabit speeds, but it is also an internet peak hour as of writing this article, I will probably need to check again in the middle of the night, but I am optimistic about the results.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/f7e42143667e22fbd90711aba4fe3279/1c045/raspierx.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;All in all, ER-X is incredibly tiny for what it does and is comparitively cheap. Paired with the tenda m5, I have full coverage of my house, no uncessessary 2.4g or 5g noise from multiple routers and also reduce the clutter on my tv console. I cannot say that I am a network expert, but I think this setup is both affordable and performant. Having the flexibility to configure whatever the heck you want is a nice add on too. &lt;/p&gt;
&lt;p&gt;Special mention to &lt;a href=&quot;https://twitter.com/kenleesm&quot;&gt;Ken Lee&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/jonathanlimsc&quot;&gt;Jonathan Lim&lt;/a&gt; who advised me on whether I really needed an enterprise grade router.&lt;/p&gt;
&lt;p&gt;Update on measurements taken during off peak timings:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/ceecdae147aa5ba0d0b876e434dc15f0/b9e4f/fast.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;I am getting 300 Mbps on wifi while seated next to my first AP. Is this a stepdown from orbi’s maxspeed? yes. Is this still fast enough for me? yes.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[AI for FE devs (Part 2)]]></title><description><![CDATA[Part 1 here This article is a complement to my presentation in React Knowledgable. Now that we know how to approach problems, we can look at…]]></description><link>https://blog.tenzhiyang.com/2019-08-01-ai-for-fe-2/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-08-01-ai-for-fe-2/</guid><pubDate>Thu, 01 Aug 2019 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;&lt;a href=&quot;https://blog.tenzhiyang.com/2019-07-31-ai-for-fe/&quot;&gt;Part 1 here&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This article is a complement to my presentation in React Knowledgable.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now that we know how to approach problems, we can look at what’s available. For this section I want to go through some big idea explanations of algorithms and what kind of input/output you can expect from and their implementation from the web. The goal here is that whenever you see a problem that you want to solve with AI, you have a rough idea what to Google, and how to transform your data to pass into these algorithms.&lt;/p&gt;
&lt;h2&gt;Fantastic algorithms and how to use them&lt;/h2&gt;
&lt;h3&gt;Markov Process (Markov Models)&lt;/h3&gt;
&lt;p&gt;Part 1 made use of a &lt;code class=&quot;language-text&quot;&gt;Markov Chain&lt;/code&gt;. The main idea for how Markov Processes work is that the future state is based on the current state and nothing more. We can define what is the “current” state like how we did with the text generation, where we defined the “current state” as the recent two words to predict the next word. Markov Models are great if you have lots of data with relatively few “states”. So how do we define what is a ‘state’? Basically the very thing that you want to be predicted from a Markov model. Since the English language has 171,476 unique words (according to a Google search), it is a relatively small number of states. This is why Markov models are used very often in text prediction, such as those in your android and iphone keyboards.&lt;/p&gt;
&lt;p&gt;Implementation details may vary, but the general idea is to take a series of &lt;code class=&quot;language-text&quot;&gt;A -&amp;gt; B&lt;/code&gt; (A leads to B, or mathematically, A implies B) and passing it into a Markov Process. If you are dealing with something like a source text, where the states can be very long strings like &lt;code class=&quot;language-text&quot;&gt;A -&amp;gt; B -&amp;gt; C -&amp;gt; ... -&amp;gt; Z&lt;/code&gt;, you can just pass the whole data into some Markov chain library. Another example that isn’t text is something like weather prediction. If it has been raining today, it will be less likely to rain tomorrow. So you can just pass the weather over the past 10 years or so in the form of &lt;code class=&quot;language-text&quot;&gt;sunny, cloudy, rain, rain, sunny, ..., sunny&lt;/code&gt; into the Markov process. After the data transformation is done, given the current weather, you’ll get a prediction of what’s the next weather. &lt;/p&gt;
&lt;p&gt;As you might observe, real life is not so obvious that you can predict X given Y and get a correct prediction all the time. Hence Markov processes are very useful for problems that give you a list of what’s the probablity of things happening after Y has happened, and a user makes an informed decision. Alternatively it can also be used for things where randomness is a bonus side effect, such as in the text generator, where I just chose a random word from the list of X predicted from Ys.&lt;/p&gt;
&lt;h3&gt;K Nearest Neighbours (KNN)&lt;/h3&gt;
&lt;p&gt;This algorithm is a very simple one. Going back to our 2-dimensional space of data, we plot all these points on the graph, and then we add a label to each one. There’s no calculations being done during the “training” time, it’s just plain data entry. So when we actually need to classify a new point, what we do is that we iterate through all the data, and then we find the K (some constant that you will input) nearest neighbours, using a simple straight-line (Euclidean) distance calculation. To use a KNN library, you should really only need to pass in the parameter and the label. Just note that some libraries do not accept negative parameter values.&lt;/p&gt;
&lt;p&gt;The great thing about KNN is that adding data really doesn’t cost anything at all, as there’s no computation. KNN scales linearly with large number of data, and large number of parameters, and this calculation is only done during the categorising process.&lt;/p&gt;
&lt;h3&gt;Random Forest Classification&lt;/h3&gt;
&lt;p&gt;Random forests comes as a variant of &lt;code class=&quot;language-text&quot;&gt;decision trees&lt;/code&gt;. A decision tree is built using some source dataset, split based on some rules based on the parameters. Each split that we make creates a “branch” with some probabilites on each branch. Eventually all branches have smaller branches until we form what looks like an upside down tree: &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/d3e1067cf318eb8febb657cad57be757/c739e/dectree.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;A random forest is just the same process, but splitting the data into random subsets of parameters, and creating a separate tree for each of them, hence the term Random Forest Classification. The algorithm will then return a list of different classifications with some number from 0-1, this number is known as the &lt;code class=&quot;language-text&quot;&gt;confidence&lt;/code&gt; for the purpose of this blog post, you can think of confidence as exactly what it sounds like: how confident the algorithm is that this is the classification.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/6692c4276cc9790d3bf0b66d620f0c3b/c739e/randomforest.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;One great thing about Random Forest Classification is since you have a list of results of different confidences, if you have non-mutually exclusive groups to classify into (eg: it can be &lt;strong&gt;both&lt;/strong&gt; raining and sunny), you can choose to accept results above some level of confidence. Random Forest algorithms normally take in an array of parameters and an array of output. The parameters need not be the same length, but parameter order matters if you want fairly accurate (greater than 90 percent) results. The output order doesn’t matter for most implementations.&lt;/p&gt;
&lt;h3&gt;Neural Networks&lt;/h3&gt;
&lt;p&gt;The absolute coolest kid on the block, nerual networks has already been explained in a previous talk, but the general idea is that we are trying to simulate how the brain works. Starting from a neuron, each neuron takes in a large number of inputs, multiply each input by some number (called weights, which define how important each input is), sums these inputs together, runs a type of mathematical function (called activation function) such that the result is between 0-1. The output of this neuron is then passed to another neuron, and so on and so forth. Each group of neurons that passes on to the next group of neurons is known as a “layer”. See how similar a NN neuron is to a brain neuron:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/ac83c7e7d2ce969673ce8b48fb237ca1/c739e/neuron.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The training algorithm will take in some input and pass it through the neural network, with all the weights randomised, and gets a prediction, which can be one or more numbers, depending on how many labels you want. It will then use the correct label and compare with the current label and tweak the weights such that the next time we pass the input, we are more likely to get a result closer to the correct label. We don’t alter the weights such that it definitely gives us the correct label because we do not want the neural network to only work on the training set, we want it to work in real life scenarios with ambiguous input that is similar but not exactly the same with the dataset.&lt;/p&gt;
&lt;p&gt;I don’t recommend training your own neural network when you’re just beginning as you need a huge amount of data (at least 500 samples of each label) and creating a neural network is really more of an art than science. It takes experience and some intuition to really know how many layers and how big a layer you want for a neural network. However, there’s proably a pre-trained model that solves your problem, and using it should be easy enough, just that every model has different requirements. You’ll need to read their respective documentation to use them.&lt;/p&gt;
&lt;h3&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;p&gt;The idea of Convolutional Nerual Networks is simple in implementation but not very intuitive to understand. Basically for an image, each pixel is a parameter (or rather, 3 or 4, with Red Green Blue and transparency values). You can &lt;strong&gt;technically&lt;/strong&gt; pass in the entire image into a Neural network, but it usually doesnt work out well, as a normal neural network does not understand the relations between pixels. Another problem is that having each pixel as a parameter is too large an input and would take forever to train.&lt;/p&gt;
&lt;p&gt;This is where the convolutional layer and pooling layer comes in. Basically the convolutional layer applies a filter, which transforms &lt;code class=&quot;language-text&quot;&gt;n * n&lt;/code&gt;pixels (a square) into 1, using some matrix manipulation mathemagic. The pooling layer comes immediately after the convolutional layer, and its job is to reduce the input space further, by taking the max or the average value in &lt;code class=&quot;language-text&quot;&gt;n * n&lt;/code&gt; pixels. These are called &lt;code class=&quot;language-text&quot;&gt;Max Pooling&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;Average Pooling&lt;/code&gt;. Max pooling by nature tends to reduce noise in the data while Average pooling is a simple &lt;code class=&quot;language-text&quot;&gt;dimension reduction&lt;/code&gt; (reducing the input size by &lt;code class=&quot;language-text&quot;&gt;x&lt;/code&gt; dimensions). The two layers together will be inserted in between some layers of a nerual network. It is possible to have more than one such convolution+pooling layer, but that increases computation time.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/073155acbd5405378a4882395a963ac7/c739e/cnn.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;There are plenty of pre-trained convolutional neural networks that recognises objects, animals, drawings or even individual hand written characters, so if you just need to identify what is inside an image, you can leverage on the existing neural nets to classify what you need. For example, during the most recent hackathon I wanted to know if the user has drawn a circle, square or diamond, my team used Google Quickdraw’s net to identify what it was and re-draw a perfect circle square or diamond in its place. I did some additional neural net on top of that, which causes the lag, but you can try it &lt;a href=&quot;https://hackathon.flowdi.tenzhiyang.com/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Recurrent Neural network&lt;/h3&gt;
&lt;p&gt;As you might notice from the Neural networks and CNN sections, the way we simulate the human brain may be a bit simplified, and RNN aims to improve on one aspect. Recurrent Neural Networks aim to add a “short term memory” by making each neuron pass its output back to itself, for the next computation. This allows information to persist inside the network where the previously input data will affect the next few predictions. There is a commonly used variant of RNN called Long Short-Term Memory (&lt;code class=&quot;language-text&quot;&gt;LTSM&lt;/code&gt;) network which works differently but produces better results.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/eccf8bf44a10fd21843ce203fd146772/c739e/recc.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The nature of having some form of short term memory makes RNN especially useful for things like speech recognition, language modeling, image captioning or even some time-based data (known as &lt;code class=&quot;language-text&quot;&gt;time-series&lt;/code&gt;) such as stock price of a company. I haven’t really used RNN yet, but it’s good to know what its capable of.&lt;/p&gt;
&lt;h3&gt;Transfer Learning&lt;/h3&gt;
&lt;p&gt;Earlier I mentioned about not training your own neural network because it (probably) already exists, but here’s something we can do to make a neural network solve more domain specific problems. Transfer learning is basically using an existing neural net, and training the top (few) layers of the neural net, so that you retain the pattern-recognition capabilities of your neural network, but optimised to your specific problem. There are plenty of tutorials and methods differ between various libraries, so I won’t go into the details of its implementation.&lt;/p&gt;
&lt;p&gt;However, one interesting take on transfer learning was something I learnt from the &lt;a href=&quot;https://www.tensorflow.org/js/tutorials/transfer/image_classification&quot;&gt;tensorflowjs transfer learning tutorial&lt;/a&gt;. Essentially what this method does is that it takes a representation (they call it an &lt;code class=&quot;language-text&quot;&gt;activation&lt;/code&gt;) of the neural network, and uses it as the parameter of a K-Nearest Neighbour. I asked some data scientist friends of mine and they have confirmed that this is not really a traditional implementation of transfer learning, although it’s a fascinating idea.&lt;/p&gt;
&lt;p&gt;The idea behind this concept is perhaps best explained in an example. &lt;a href=&quot;https://httpserve.tenzhiyang.com/imageRecognition/&quot;&gt;This Project&lt;/a&gt; uses image net, which is trained to recognise object from images. Even if the net is not trained to recognise something, for example which direction my head is facing, when the image changes drastically, there are some neurons in the network that will have a high value and some neurons that won’t change at all. Therefore knowing which are the “triggered” and “non-triggered” neurons as parameters, we are able to categorise new objects using KNN, using a model that learnt from another data set.&lt;/p&gt;
&lt;h2&gt;Example Projects&lt;/h2&gt;
&lt;p&gt;Here are a few projects that I’ve been doing in the past couple of weeks, where each project takes about a couple of hours to build, I do try to add something on top of just consuming the API, so that you can catch a glimpse of how I make use of existing tools to do something else instead of following tutorials or documentation to solve a problem that someone has already solved before.&lt;/p&gt;
&lt;p&gt;All of these projects are running entirely on the front end, they are all static sites served off a simple HTTP server.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/gumshoos/&quot;&gt;Gumshoos&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time taken: 45 mins, of which maybe 30 was looking for a super simple no frills implementation of markov chains.&lt;/li&gt;
&lt;li&gt;Libraries: &lt;code class=&quot;language-text&quot;&gt;nlp_compromise&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/imageRecognition/&quot;&gt;Image Recognition&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time taken: 1.5 hours, half of the time spent following the google codelab tutorial&lt;/li&gt;
&lt;li&gt;Libraries/models: &lt;code class=&quot;language-text&quot;&gt;tensorflowjs&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;mobilenet&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;knn-classifier&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/greenScreen/&quot;&gt;Green Screen&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time taken: 1.5 hours&lt;/li&gt;
&lt;li&gt;Libraries/models: &lt;code class=&quot;language-text&quot;&gt;tensorflowjs&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;body-pix&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;jscolor&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Body-pix is a model that is used for segmenting body parts in an image. I made use of a function &lt;code class=&quot;language-text&quot;&gt;toMaskImageData&lt;/code&gt; which basically gives me a “silhouette” of a person in the image. I use a few composite methods available from the canvas context api (webcam input, overlay silhouette with &lt;code class=&quot;language-text&quot;&gt;destination-in&lt;/code&gt;) to remove the background. Then green screen in the background is really just setting a background color. Theoretically we can overlay the greenscreen effect over anything, as long as they’re both in the same webpage.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/mocap/&quot;&gt;Mocap&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time taken: 1 hour&lt;/li&gt;
&lt;li&gt;Libraries/models: &lt;code class=&quot;language-text&quot;&gt;tensorflowjs&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;posenet&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I really didn’t do anything for this. Passing in the webcam-feed into &lt;code class=&quot;language-text&quot;&gt;posenet&lt;/code&gt;, I receive a list of bodyparts followed by their x and y coordinates and a confidence score. Next, I drew red squares where each body part was. I can go a step further to remove those body parts with low confidence, but that’s just from analysing the input and output and using some constant value that works well enough.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/jojoPose/&quot;&gt;Jojo’s Bizarre Poses&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Time taken: 1 hour (excluding the time required to build mocap)&lt;/li&gt;
&lt;li&gt;Libraries/models: &lt;code class=&quot;language-text&quot;&gt;tensorflowjs&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;posenet&lt;/code&gt; &lt;code class=&quot;language-text&quot;&gt;knn-classifier&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is like a combination of &lt;code class=&quot;language-text&quot;&gt;mocap&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;image recognition&lt;/code&gt; projects above, but since the KNN classifier I used was from the tensorflow libraries, it takes in a certain data type &lt;code class=&quot;language-text&quot;&gt;tensors&lt;/code&gt; as input. I had to modify the data to fit the KNN classifier.&lt;/p&gt;
&lt;h2&gt;Some useful tools&lt;/h2&gt;
&lt;p&gt;What I went through in the past two blog posts cannot possibly cover all the use cases you might want to work on, so I’ve decided to compile a list of useful libraries and data sets that you can use in your own personal projects.&lt;/p&gt;
&lt;h3&gt;Some famous Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Brown Corpus&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Known formally as the &lt;strong&gt;Brown University Standard Corpus of Present-Day American English&lt;/strong&gt; this dataset contains about 500 samples of English-language text and is used frequently in NLP projects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quick draw&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remember &lt;a href=&quot;https://quickdraw.withgoogle.com/?locale=en_US&quot;&gt;this game&lt;/a&gt;? It was a data collection exercise, and they’ve open sourced all the drawings together with the draw order!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ImageNet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A database of pictures that are labeled by crowd-sourcing, this dataset contains more than 20,000 categories, with bounding boxes to highlight the objects being labeled.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MS-COCO&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Common Objects in Context&lt;/strong&gt;, this is a dataset of common objects are taken from every day scenes. Instead of bounding boxes like in ImageNet, they have object segmentation, just like my green-screen example, but for objects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MNIST&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MNIST database (&lt;strong&gt;Modified National Institute of Standards and Technology&lt;/strong&gt; database) is a database of handwritten digits and is used very frequently to practice creating handwritten character recognition algorithms.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Free Spoken Digit&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the MNIST of Speech recognition basically.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Free Music Archive&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Just an archive of royalty free music!&lt;/p&gt;
&lt;h3&gt;Some famous Libraries&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ML.js&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The scikit AI toolset, now replicated in js, has lots of learning algorithms and some useful data transforming utilities.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compromise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A fast NLP library which helps format text. Uses some outdated techniques in favour of speed over accuracy, aims to solve 90% of use-cases.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflowjs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A re-write of the famous tensorflow in js. There’s quite an established community supporting and I can find very good models written for or compatible with Tensorflowjs&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kerasjs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A fairly popular neural network library, the models produced by Keras can be imported into Tensorflowjs also.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ML5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another Nerual Network library. It is actually a high-level interfact for Tensorflowjs. I’ve heard that ML 5 has more beginner friendly jargon and documentation, but I don’t have experience with it yet.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[AI for FE devs (Part 1)]]></title><description><![CDATA[This article is a complement to my presentation in React Knowledgable. Introduction Artificial Intelligence is all the rage now, with the…]]></description><link>https://blog.tenzhiyang.com/2019-07-31-ai-for-fe/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-07-31-ai-for-fe/</guid><pubDate>Wed, 31 Jul 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;strong&gt;This article is a complement to my presentation in React Knowledgable.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Artificial Intelligence is all the rage now, with the craze starting from Deepmind’s alpha go. Although I hate people for misusing the terms “artificial intelligence” and “machine learning” for things that do not need AI and ML to solve, I do think that using AI reduces the time taken to solve certain problems, and also can add some “WoW factor” to an idea. I hope with this article (and subsequent presentations) people will understand what kind of problems that AI can solve, and maybe make AI more approachable so that the overall quality of hackathon projects improve.&lt;/p&gt;
&lt;p&gt;First off, a disclaimer: &lt;strong&gt;I am by no means an expert in AI. I do have an undergrad level understanding of artificial intelligence, but I was never great at it.&lt;/strong&gt; Feel free to correct me on any wrong concepts that I write in this article. I will also try to explain AI in English and not technobable, so for the real AI masters out there, you might cringe at me using wrong analogies or even butchering terminologies in an attempt to make AI easier to understand.&lt;/p&gt;
&lt;p&gt;My motivation for this article stems from two comments that irks me. These (paraphrased) comments come from people who I think are smarter and better engineers than me. We are all front end engineers, and I can understand why they said it, but all these were in the context of Hackathons, and I feel that it’s a shame for people to be limited in implementation just because AI seems so daunting.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can use my idea because I don’t know how to build it&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This came from a very creative person whose idea not only allowed me to win the second prize, they also won a consolation prize in their idea. People say ideas are cheap. That’s half true. Yes, there is likely someone else who stumbled across the same idea and has the skills to implement it, but really most of the time we already have the capacity to build this idea, we just don’t know it yet. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wish I did more AI related things in University, I feel like I missed out&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is &lt;strong&gt;the&lt;/strong&gt; comment that prompted me to come up with this article. I always wanted to present something about AI to my team, given that I do have &lt;em&gt;some&lt;/em&gt; understanding of AI, but someone else did an introduction to Neural Networks that was more detailed than what I could come up with and I felt that that person would be more suited to carry on doing AI related talks. However from this comment I realised that &lt;strong&gt;because&lt;/strong&gt; I am kinda shit in AI, but yet I know how to &lt;strong&gt;use&lt;/strong&gt; AI libraries, I can add value to this topic. &lt;/p&gt;
&lt;p&gt;So here’s what I hope people will gain from this article:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approaching Problems with AI&lt;/li&gt;
&lt;li&gt;What algorithms are good to google&lt;/li&gt;
&lt;li&gt;What can we achieve with just a surface level knowledge of AI&lt;/li&gt;
&lt;li&gt;Famous APIs that we can use for the next hackathon/prototype of things that we wanna work on&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s what I will &lt;strong&gt;not&lt;/strong&gt; go through&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How Neural networks work&lt;/li&gt;
&lt;li&gt;How to build an AI from scratch&lt;/li&gt;
&lt;li&gt;How to beat the no.1 go player with AI&lt;/li&gt;
&lt;li&gt;How famous algorithms work&lt;/li&gt;
&lt;li&gt;How to win a hackathon with AI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Mostly because I don’t know the answers to any of the above.&lt;/p&gt;
&lt;p&gt;Now this article will be written with FE developers in mind, with introductions to Javascript libraries. Many people would say that there are a lot of better languages for AI development and I entirely agree. There are some benefits to do AI in Javascript. Firstly, there’s always the Nodes argument: your entire stack can be the same language. Whatever flavour of JS you are using, you can easily switch over from one to another. This is great for a quick prototype and/or proof of concept to show that your idea even works in the first place. Secondly for the users, you can have the entire script running on the browser. You can write some application once and run it anywhere, you don’t have to worry about scaling as all the computation is running on the user’s device. This also means you can have your application runnning offline, with PWAs. I think this is one of the strongest points we have for AI running on browsers. Finally, in terms of performance, JS is not much behind python, in some cases it might even been faster. It’s really only a matter of time before the tools available for JS catches up to Python as the JS community grows larger and larger.&lt;/p&gt;
&lt;h2&gt;What is AI&lt;/h2&gt;
&lt;p&gt;There are a few definitions of what AI is, and many of us get confused about the terminology. What is Artificial Intelligence and what is Machine Learning? Are they even different? The most sensible definition I found is that Artificial Intelligence is &lt;code class=&quot;language-text&quot;&gt;Simulation of human thought processes in machines.&lt;/code&gt; I find this to be very accurate. AI can be as simple as a bunch of if-else statements (that’s essentially a &lt;code class=&quot;language-text&quot;&gt;decision tree&lt;/code&gt;) or something as complicated as simulating a brain (&lt;code class=&quot;language-text&quot;&gt;neural nets&lt;/code&gt;). Machine learning is a subset of artificial intelligence, and basically what ML does is to write a program to find patterns in data, without being explicitly written which pattern to recognise.&lt;/p&gt;
&lt;p&gt;Now in my own understanding, we really just want to solve three different types of problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Categorise things&lt;/li&gt;
&lt;li&gt;Make Decisions&lt;/li&gt;
&lt;li&gt;Generate things&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These groups of problems are actually not how actual AI-savvy people group them, but I find these approach of grouping problems easier to use to visualise solving some problem with AI.&lt;/p&gt;
&lt;p&gt;These problems have to be represented by some form of data, so that’s something that usually requires some level of intuition and experience, but after a while you might start recognising certain methods people use to represent different data. For now, let’s assume that all real world data come in the form of numbers stored in some variables.&lt;/p&gt;
&lt;h2&gt;Categorising problem&lt;/h2&gt;
&lt;p&gt;For the first type of problems, we can imagine a mathamatical function. Suppose we want to guess: given a Merge Request (Pull Request) how many people will actually do a MR vs giving a “free” &lt;code class=&quot;language-text&quot;&gt;LGTM&lt;/code&gt;? In this case, maybe a point of interest would be the length of the MR by lines of code changed. We can expect that with more lines, the more likely people will &lt;code class=&quot;language-text&quot;&gt;LGTM&lt;/code&gt; it without reading it. Now maybe a second point of interest that we want to do is how many files have been changed. Now if there’s very few file changes, the code change is easier to understand, and people will tend to actually review the code compared to MRs with lots of file change. So we end up with something like this:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/ac238eb1b7ab61a0f7f2463db59bb051/b9e4f/datapoint.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;So what we want our algorithm to do is to draw some line such that everything above the line belongs to &lt;code class=&quot;language-text&quot;&gt;LGTM&lt;/code&gt;-ed category, and everything below the line belongs to the &lt;code class=&quot;language-text&quot;&gt;actually been reviewed&lt;/code&gt; category. So we pass these data into some categorising function and it will generate a best-fit line. Now whenever we want to categorise a new MR, we just plot it on the graph and check its relation to the said line like so:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/f8c3ca700bab95a2c5dd2607e5c5499a/b9e4f/newdatapoint.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;But that is just two variables, what if we want to have a third variable? Like for example, how near is the deadline? The nearer the deadline, the more likely someone will skim through it, the further it is, the more likely someone will read it through. Well for 3 parameters, we can actually visualise it as a point in a 3-dimensional space. Instead of a line, we will separate the dataset with what we call a plane (that is, a 2-d surface that extends indefinitely), like so:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/b06ebc757eb5b3db32071b04750708fd/c739e/plane.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;If we add another variable? Easy! Make it four dimensional, or five, or six or however many variables you need. In ML we call these variables &lt;code class=&quot;language-text&quot;&gt;parameters&lt;/code&gt; and we call the thing separating the different categories the &lt;code class=&quot;language-text&quot;&gt;hyperplane&lt;/code&gt; (subspace whose dimension is one less than that of its ambient space). Not all categorising algorithms work like this, but this is a way to visualise your problem that you want to solve.&lt;/p&gt;
&lt;h2&gt;Decision making problem&lt;/h2&gt;
&lt;p&gt;Next kind of problem, we have decision making. Essentially, given some number of choices, we want to be able to choose the most desirable option. A case study for this kind of problem would be something like a Tetris playing bot. Let’s establish some rules of the Tetris that we’re going to analyse. This will keep our problem scope smaller and easier to analyse.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;your score is how many lines you clear (one line, one point)&lt;/li&gt;
&lt;li&gt;how fast you make the move doesnt matter&lt;/li&gt;
&lt;li&gt;you only know the current piece, and the current state of the game&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So let’s think, how we would want &lt;em&gt;a robot&lt;/em&gt; to play this game. I added the words &lt;em&gt;a robot&lt;/em&gt; there for one simple reason. We don’t want to care about things that will affect us emotionally. We are aware that the game gives us four points if we clear four lines in one go, and four points if we clear four lines individually, but for us it will feel so much more satisfying if we were clearing four lines at one go. Similarly, we don’t want the robot to care if making the optimal move makes the state of the game look “ugly”, we only care if its the best move at the current point in time.&lt;/p&gt;
&lt;p&gt;What we’ll do is given a current state of the game, we want to place the moves in every single possible(legal) ways and then use some method to gauge if a move is good or not. So what is this method? We will take the state of the game, and then come up with some numbers to represent a desirable trait or an undesirable trait. We will then sum up the desirable traits, and minus the undesirable traits to get a score. These traits are known as &lt;code class=&quot;language-text&quot;&gt;heuristics&lt;/code&gt;. Heuristics give us some interpretation of how good a decision is, and more importantly, they are usually fast to calculate. Of course, some traits are more important than others, so we need to multiply them by some constant value. These constant values are called &lt;code class=&quot;language-text&quot;&gt;weights&lt;/code&gt;. We can throw the weights into some classification algorithm to optimise what number is better, so we don’t need to care about what the numbers are for the sake of this article.&lt;/p&gt;
&lt;p&gt;Let’s think of the first heuristic we care about. Of course we want to clear lines, and even though clearing more lines doesn’t give us more points, but it makes the game last a little bit longer, so let’s just count if there are lines cleared or not this turn:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/b5b202697ba2915bed68d75bad0c7596/c739e/linesclear.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Next thing, we want the robot to play as many moves as possible, so that it has the most chances to clear lines, we don’t want the bot to make an optimal move it that ends the game right now. So let’s take something like, the current height of the game like so:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/ba095da270fa5c0e70f815fcd35a149c/c739e/height.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Now this is good, but suppose we have just one line that’s really tall but doesnt lose immediately, that’s not entirely bad right? So maybe we can do something better. instead of just height, let’s sum up the height of each individual column, like this:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/2cc87472321eb3eb7805522ad7be660e/c739e/colheight.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Finally the biggest bane of Tetris is to have a “hole” right? it’s bound to happen but we want to reduce the number of holes. Lets count the number of holes and then multiply it by a some negative weight:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/b4f9d1896a761a1cd7acfb2667b239bd/c739e/holes.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;But it’s not neccessarily just holes, overhanging piece will eventually create holes (assume no T-spins and no “sliding” at the bottom) so we want to count that as well. And a hole with more overhang is worse than a piece that just overhangs. So let’s do something like this:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/5780c3d471812030d953998ea2c1733f/c739e/overhang.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;If we look at the previous two, isn’t the hole count really some subset of the overhang count? So maybe we can do something better. How about if starting from bottom, we add a count everytime we go from “solid” to “gap” this way we can actually detect when there are a lot of small gaps, as small gaps take more moves to solve but big gaps could be solved easier.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/2f51a4fccdefb0b72239ebc5ecff6e8d/c739e/bumpiness.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;But I think we can do better. This heuristics takes care of how many gaps there are in the same column, but it doesn’t really have a big picture representation of how sever a gap is. A gap with many bends is much harder to solve than a ‘smooth’ gap. A heuristic that can take this into consideration could be something like how many 90 degree bends there are in the state.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/b3e97bff0c717846a1295184fc097d67/c739e/bends.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Summing up the heuristics and then multiplying by some constant weights to them, we have something like:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;c_1 * linesCleared + (-1 * c_2) * sumColHeight + (-1 * c_3) * numBends&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So now we have a bot that analyses Tetris like how an almost perfect human would, except the analysis needs to be weighted. All details aside, we can look for some algorithm that takes in &lt;code class=&quot;language-text&quot;&gt;c_1, c_2, c_3&lt;/code&gt;, runs a game of Tetris, gets the score, modifies &lt;code class=&quot;language-text&quot;&gt;c_1, c_2, c_3&lt;/code&gt; a little bit and runs the game again. rinse and repeat until &lt;code class=&quot;language-text&quot;&gt;c_1, c_2, c_3&lt;/code&gt; is fairly stagnant, or you don’t have time to train anymore. In unviversity my teammates used a library which provided something called &lt;code class=&quot;language-text&quot;&gt;particle swarm optimisation&lt;/code&gt; which brought my team’s AI score up from ten thousands to millions just by running a optimisation algorithm on the weights.&lt;/p&gt;
&lt;h2&gt;Content Generation Problem&lt;/h2&gt;
&lt;p&gt;For the final type of problem, unfortunately there are too many different ways for diffent domains. Generating an Image is quite different from generating sounds or text. There is some headway being made in the form of neural networks which pit two neural networks against each other, but that’s a whole new article and presentation. However for text generation, there are some probability-based state machines (known as &lt;code class=&quot;language-text&quot;&gt;Markov model&lt;/code&gt; or &lt;code class=&quot;language-text&quot;&gt;Markov process&lt;/code&gt;) that is easy to program, and also gives us reasonable results.&lt;/p&gt;
&lt;p&gt;I won’t go through the process of writing a &lt;code class=&quot;language-text&quot;&gt;Markov model&lt;/code&gt; but the general idea for text generation will be something like. “What is the most likely word to appear next, given that the words before are &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;b&lt;/code&gt; (and &lt;code class=&quot;language-text&quot;&gt;c&lt;/code&gt; and &lt;code class=&quot;language-text&quot;&gt;d&lt;/code&gt; etc etc etc..)?” So what we will do is to find a markov model, and find sufficiently big dataset. An example that I want to use is something like a Donald Trump text generator. &lt;/p&gt;
&lt;p&gt;First things first, I need a dataset. In the domain of Natural Language Processing, we call a file with lots of sentances a corpus, so I found &lt;a href=&quot;https://github.com/ryanmcdermott/trump-speeches&quot;&gt;1mb worth of trump speeches&lt;/a&gt; from &lt;a href=&quot;https://twitter.com/ryconoclast&quot;&gt;@ryconoclast&lt;/a&gt;. We can do some cleanining up of the data, like changing all text to lowercase, and grouping words that come from the same word together(&lt;code class=&quot;language-text&quot;&gt;stemming&lt;/code&gt;, eg: consultant, consulting, consults -&gt; consult) but I won’t be using it for my example, just to showcase how decent the results are without much effort.&lt;/p&gt;
&lt;p&gt;After finding &lt;a href=&quot;https://medium.com/@corrigan1247/how-to-imitate-trump-with-markov-chains-8224877dcf69&quot;&gt;a tutorial on how to do exactly trump speech generators&lt;/a&gt;, I passed in the entire text file into the function, added some checks to make my generated content at least 20 words long, and this is my result:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/493d544d615b06d81ba9320aa313b80e/ef8bc/2gram.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Not too bad, but a little bit random right? If you did read through the tutorial on building Markov chains, you might guess why. If not, the general idea is that we are only looking at some word &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt; and getting a list of words that come after &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;, and choosing a word randomly from there. This is kinda funny but not really as fluent as we like it to be. So I did some modifications and made it such that we get a sequence of &lt;code class=&quot;language-text&quot;&gt;x&lt;/code&gt; number of words to generate the next one. So here’s one where we look at the previous 2 words:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/f73270bd07bba4e4b81803120f474bcf/67648/3gram.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Pretty good! increasing the number of words we reference:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/2b8d20ea7d6509aeda829807440bb282/22492/4gram.png&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/56da2c78e8a0a94c2008364c859f63c1/26532/5gram.png&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/66c7aab144b3ab93b80b8514581923b4/ab8e7/6gram.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Woah, that last one seems a bit too on the nose doesn’t it? That’s because it’s directly lifted from the source.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/f9c9fdd09306e8d0b8efe0ccf6d8b99b/b9e4f/corpus.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;So here lies a decision we must make. The more fluent we want our robot to be, the more previous words we want to use to predict the next words. However the more words we use, the more we directly lift from the source text. I find that a good number of previous words to look back on is between 1 to 3. You can try the &lt;a href=&quot;https://httpserve.tenzhiyang.com/gumshoos/&quot;&gt;gumshoos speech generator&lt;/a&gt; with 2 previous words (bi-gram).&lt;/p&gt;
&lt;p&gt;That’s three different case studies of analysing three different AI solvable problems, (mis)quoting portal: now you’re thinking in AI, with this Part 1 I hope you manage to gain some insight on how people approach real world problems and breaking them down into numbers, then trying to solve it in one of the three above methods. It comes with practice and there are definitely more than one way to categorise, make decisions or generate problems, so keep at it, get to know more AI tools available and you can evantually make great AI applications in a short period of time!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[A race condition in react-redux]]></title><description><![CDATA[TLDR: file that used to be on code base became an API, listen to value changes on redux and send another fetch if neccessary  Recently I…]]></description><link>https://blog.tenzhiyang.com/2019-07-19-a-race-cond-in-react-redux/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-07-19-a-race-cond-in-react-redux/</guid><pubDate>Fri, 19 Jul 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;strong&gt;TLDR: file that used to be on code base became an API, listen to value changes on redux and send another fetch if neccessary&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;Recently I faced some problems with a component not appearing on live, despite no changes in the component’s code for maybe months! It was very frustrating as this bug only happened in live, and not in staging, not in test, not in UAT. In fact even when I ran the code locally using live data, I could only reproduce the bug maybe once every ten tries. Now this blog post is something I’m trying so maybe it will be too vague because I can’t reveal company code, but I want to try to explain big picture concept without too much details.&lt;/p&gt;
&lt;p&gt;The major problem I had was due to a site-wide refactor. We had a list of constants that we would use to decide what to show, depending on which locale it was in. Super useful! Naturally, in order to turn some features on and off, we had to change that constant via some tool, and then re-deploy to production. It works, there’s no need to touch the code base,  and now we dont need software engineers to turn on and off features. Natural progression led us to improve this design. Of course, we want to turn features on and off without waiting for another deployment, so therefore this file should just be deployed as an api! We can fetch this file as and when we need it! Brilliant!&lt;/p&gt;
&lt;p&gt;Except we just created a race condition. Turns out, some logic requires us to alter our fetch in some way due to said toggles. This resulted in my component not displaying anything. Now there were several problems in the component design, either due to legacy reasons, or maybe lack of experience. If we had the time and (mostly QA) resources, we might have wrote this component some other way, but that’s not really the root problem. We need a fix quickly, and then maybe down the road we can fix the architectural problems.&lt;/p&gt;
&lt;p&gt;Some issues with the implementation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we used the fetch parameters as keys in our redux store so that we could store multiple results from the same api&lt;/li&gt;
&lt;li&gt;fetch parameters was generated using the value from the file, which became the API&lt;/li&gt;
&lt;li&gt;the result from the file value would go through some util and would fail gracefully, when the params were to go into the fetch, we would assume nothing is wrong&lt;/li&gt;
&lt;li&gt;the generated fetch key and the key used for display were calculated at the point and time where they were needed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The best solution I could come up with in a short perood of time was to send another fetch when the value retrieved from the util changed. What this means is that if we sent the fetch on an &lt;code class=&quot;language-text&quot;&gt;UNSAFE_componentWillMount()&lt;/code&gt; (on legacy code) we should check the prop changes in &lt;code class=&quot;language-text&quot;&gt;UNSAFE_componentWillReceiveProps()&lt;/code&gt;, and then send another fetch based on the next props, compared to the currentprops.&lt;/p&gt;
&lt;p&gt;In cases where we use &lt;code class=&quot;language-text&quot;&gt;getDerivedStateFromProps()&lt;/code&gt;, we don’t have access to the current props. In this scenario, I had to make a copy of the what I need to listen to into the current state, and on next &lt;code class=&quot;language-text&quot;&gt;getDerivedStateFromProps()&lt;/code&gt; I would have a copy of the current props to compare with.&lt;/p&gt;
&lt;p&gt;I think fundamentally the issue I’m facing is probably an architecture problem, but the way I solved it is probably the fastest and easiest way I know how. Do tweet at me if you have a better quick solution though!&lt;/p&gt;</content:encoded></item><item><title><![CDATA[New blog, who dis?]]></title><description><![CDATA[So it all started out with a tweet  Nothing like telling everyone you’re going to do something to force yourself into totally doing it. So I…]]></description><link>https://blog.tenzhiyang.com/2019-07-07-new-blog-who-dis/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-07-07-new-blog-who-dis/</guid><pubDate>Sun, 07 Jul 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;So it all started out with a tweet&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/7321e9c0a3a2fecec39c39b1b8deb8c1/b9e4f/tweet.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Nothing like telling everyone you’re going to do something to force yourself into totally doing it.&lt;/p&gt;
&lt;p&gt;So I started working on this. Right now this is pretty much the default gatsby blog. There are some nice things coming over to gatsby from jekyll, the expressiveness of jsx over some templating engine is always nice. The &lt;code class=&quot;language-text&quot;&gt;automagical&lt;/code&gt; aspect of gatsby is in the source code, I just have to go and learn it.&lt;/p&gt;
&lt;p&gt;The build process on ubuntu has been far from great. None of the gatsby cli works on my ubuntu instance, maybe it’s my npm setup (I’m experimenting with asdf-vm). Right now I need to &lt;code class=&quot;language-text&quot;&gt;gatsby clean&lt;/code&gt; on my development machine, run &lt;code class=&quot;language-text&quot;&gt;gatsby build&lt;/code&gt;, commit my public folder, and host that on a http-server. Maybe I need to write some hack to make it better, maybe the solution exists, but I sure as heck couldn’t find it.&lt;/p&gt;
&lt;p&gt;I might publish another post explaining into more details what I’ve done with this blog and also the server this is running on, but for now I will focus on a few posts based on &lt;a href=&quot;https://twitter.com/devdevcharlie&quot;&gt;Charlie Gerard’s&lt;/a&gt; brilliant presentation from jsconf asia.&lt;/p&gt;
&lt;p&gt;Here’s a &lt;a href=&quot;https://httpserve.tenzhiyang.com/knnTransferLearning/&quot;&gt;sneak peek&lt;/a&gt; at one of my examples that I have in mind.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Telegram ramblings]]></title><description><![CDATA[I have made it a point to write some of my ideas down on telegram, I will need to find a sustainable solution to update this list as and…]]></description><link>https://blog.tenzhiyang.com/2019-06-29-some-telegram-rablings/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-06-29-some-telegram-rablings/</guid><pubDate>Sat, 29 Jun 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I have made it a point to write some of my ideas down on telegram, I will need to find a sustainable solution to update this list as and when I write more stuff in it, for now I’ll just write the ideas down here&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“who wrote this project” git crawler&lt;/li&gt;
&lt;li&gt;graph ui (basically the graphql of ui. write garph query in templating engine, generate ui?)&lt;/li&gt;
&lt;li&gt;slide controller using posenet&lt;/li&gt;
&lt;li&gt;shadow boxing guide&lt;/li&gt;
&lt;li&gt;Text to picture&lt;/li&gt;
&lt;li&gt;Electric ukulele visualisation&lt;/li&gt;
&lt;li&gt;Speech to text but with tone (represented with emoji?)&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Start of a new Era? maybe?]]></title><description><![CDATA[So maybe I will start blogging? This is part of learning in public from the really inspirational @swyx Here’s to many more bad blogs ahead]]></description><link>https://blog.tenzhiyang.com/2019-06-28-start-of-new-era-maybe-maybenot/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-06-28-start-of-new-era-maybe-maybenot/</guid><pubDate>Fri, 28 Jun 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;So maybe I will start blogging? This is part of learning in public from the really inspirational &lt;a href=&quot;https://twitter.com/swyx/status/1009174159690264579&quot;&gt;@swyx&lt;/a&gt; Here’s to many more bad blogs ahead&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Post SEA hackathon 2019]]></title><description><![CDATA[This also spurred me to look into my 2018 success and write a blog about that. First thing out of the way is I won nothing and it sucks but…]]></description><link>https://blog.tenzhiyang.com/2019-06-23-post-hackathon/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-06-23-post-hackathon/</guid><pubDate>Sun, 23 Jun 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This also spurred me to look into my 2018 success and write a blog about that.&lt;/p&gt;
&lt;p&gt;First thing out of the way is&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I won nothing and it sucks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;but the thing I realised about how it sucks here is that the way it sucks is a different suckage from how it used to suck when I entered hackathons. It sucks not because I didn’t win, not because I think my project was better than the other teams’ but mostly because of how hard we worked and how little results came of it. There’s a few ways of looking at this.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The idea just wasn’t presetable. I don’t really believe this is the case, as another group did something similar (but just better) and achieved great results.&lt;/li&gt;
&lt;li&gt;I am too unfamiliar with CNNs and transfer learning. This can be easily solved. I will eventually release a library that does all the heavy lifting for us.&lt;/li&gt;
&lt;li&gt;Our team composition and my delegation wasn’t good. I really think I lucked into my first team where I didn’t have to worry about whatever aspect that I delegated to. This time I think we should have maybe had one more backend and less FE devs. The AI was also kind of easy to implement maybe someone else would have been faster&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Post js conf]]></title><description><![CDATA[I’m writing this about 2 weeks after I’ve attended jsconf, so my memory will be hazy about everything. Here’s a few actionable things I want…]]></description><link>https://blog.tenzhiyang.com/2019-06-16-post-js-conf/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-06-16-post-js-conf/</guid><pubDate>Sun, 16 Jun 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I’m writing this about 2 weeks after I’ve attended jsconf, so my memory will be hazy about everything. Here’s a few actionable things I want to do after what I’ve learnt at jsconf:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Blog more.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is really not too related to jsconf, but I did read &lt;a href=&quot;https://www.swyx.io/writing/learn-in-public/&quot;&gt;this brilliant post by swyx&lt;/a&gt; and it’s crazy inspiring. I’m not him and it’s unlikely I will be as active and as motivated as him, but I think this is a good way to be some inbetween, I may not be the second swyx or the second gaow, but at least I can be the first tenzy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Play more with Tensorflow.js (and maybe git gud at AI once and for all)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.com/devdevcharlie&quot;&gt;Charlie Gerard&lt;/a&gt; made some really cool demos on stage, and after using the tensorflow.js in the hackathon (more on that next post) I realised that AI tools are so accessible now, I really should be doing more.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Participate more in twitter&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;particularly, write TILs for what I’ve learnt. Initally I wanted to do something every day, but I’m not really keeping up with it, so I will have to fine tune the frequency&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use more diverse frameworks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The talk from &lt;a href=&quot;https://twitter.com/youyuxi&quot;&gt;Evan You&lt;/a&gt; made me rethink about framework loyalty. Every framework solving a different problem is such an obvious conclusion but the fact that I never reached there on my own makes me feel that I actually have some software engineering maturing to do&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Join community events more frequently&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It was after all the super silly hackathon that introduced me to this event and also the free tickets. Not to mention my dvd logo being plastered all over the event. I feel that there’s only good to be had from being in the community events, and in the worst case, maybe I’ll help someone&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Amp Slides]]></title><description><![CDATA[Having “volunteered” to give a talk about my experience in japan, I decided to make my slides out of AMP itself. By using handlebars…]]></description><link>https://blog.tenzhiyang.com/2019-05-28-amp-slides/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2019-05-28-amp-slides/</guid><pubDate>Tue, 28 May 2019 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Having “volunteered” to give a talk about my experience in japan, I decided to make my slides out of AMP itself. By using handlebars templating engine, I wrote &lt;a href=&quot;https://github.com/Tzyinc/amp-playground&quot;&gt;a tool that converted json into slides&lt;/a&gt; if i were to continue this project, I think I would write it in some proper framework and not use the completely uneccessary templating engine.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://httpserve.tenzhiyang.com/AMPslides/&quot;&gt;See the slides&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[JS Conf Animation: random tiled lines]]></title><description><![CDATA[This entry is for the programming nerds. 10 PRINT, a book about the commandore 64 takes one line of code to demonstrate the concept of…]]></description><link>https://blog.tenzhiyang.com/2018-12-24-js-conf3/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2018-12-24-js-conf3/</guid><pubDate>Mon, 24 Dec 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This entry is for the programming nerds. &lt;a href=&quot;https://10print.org/&quot;&gt;10 PRINT&lt;/a&gt;, a book about the commandore 64 takes one line of code to demonstrate the concept of generative art. It is incredibly simple for something that looks so amazing. This is literally only just randomised tiled diagonal lines.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;400px&quot; src=&quot;https://httpserve.tenzhiyang.com/tiledLines/&quot; /&gt;</content:encoded></item><item><title><![CDATA[JS Conf Animation: Cantor Set]]></title><description><![CDATA[The idea for this entry was to hopefully spark a math nerd in the organisers.]]></description><link>https://blog.tenzhiyang.com/2018-12-23-js-conf2/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2018-12-23-js-conf2/</guid><pubDate>Sun, 23 Dec 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The idea for this entry was to hopefully spark a math nerd in the organisers.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;400px&quot; src=&quot;https://httpserve.tenzhiyang.com/cantor/&quot; /&gt;</content:encoded></item><item><title><![CDATA[JS Conf Animation: DVD logo]]></title><description><![CDATA[The next few posts are pull requests for jsconf asia’s home page, hopefully they accept it and I get free tickets! The inspiration for this…]]></description><link>https://blog.tenzhiyang.com/2018-12-22-js-conf/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2018-12-22-js-conf/</guid><pubDate>Sat, 22 Dec 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;The next few posts are pull requests for jsconf asia’s home page, hopefully they accept it and I get free tickets!&lt;/p&gt;
&lt;p&gt;The inspiration for this animation comes from the dvd bouncing logo meme.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;400px&quot; src=&quot;https://httpserve.tenzhiyang.com/dvdlogo/&quot; /&gt;</content:encoded></item><item><title><![CDATA[Sew Many Images]]></title><description><![CDATA[100% JavaScript implementation for CSS image sprite generator. Use it now!]]></description><link>https://blog.tenzhiyang.com/2018-09-10-sew-many-images/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2018-09-10-sew-many-images/</guid><pubDate>Mon, 10 Sep 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;100% JavaScript implementation for CSS image sprite generator.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.npmjs.com/package/sew-many-images&quot;&gt;Use it now!&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Post hackathon 2018]]></title><description><![CDATA[I’m writing this in 2019, although it will be filed in 2018 for my life’s chronological order, but this is more of a reflection on what was…]]></description><link>https://blog.tenzhiyang.com/2018-08-05-post-hackathon/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2018-08-05-post-hackathon/</guid><pubDate>Sun, 05 Aug 2018 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/97ad3ab868c16f75cc43c07e2597d0e4/c739e/hackathon.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;I’m writing this in 2019, although it will be filed in 2018 for my life’s chronological order, but this is more of a reflection on what was different here as well as what I think was amazing here.&lt;/p&gt;
&lt;p&gt;Now apart from the incredible luck of having 1) a good idea delivered right to my team and 2) the really sicko teams were out of topic there’s some really good thing we can take from here.&lt;/p&gt;
&lt;p&gt;Firstly the fact that our idea was presentable. I think the problem with my 2019 idea was that unless we had a flawless implementation, we couldn’t demonstrate the idea properly. Our 2018 demonstration sucked ass but the idea was very obvious, the presentation itself was also very easy to understand what was going on at the end.&lt;/p&gt;
&lt;p&gt;Secondly, the project was an incredibly small scope. using a deterministic solution is really the key here. Having something that wasn’t up to chance means debugging is really do-able.&lt;/p&gt;
&lt;p&gt;Finally I think complexity of UI made this project take just a little bit too much time. UI has to look good, but it has also have to be simple.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[@sg_carparks_bot]]></title><description><![CDATA[Sg carparks is a telegram bot that I developed to help find nearby parking lots. You send the bot your current location, and it brings up…]]></description><link>https://blog.tenzhiyang.com/2017-10-16-sg-carparks/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-16-sg-carparks/</guid><pubDate>Mon, 16 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Sg carparks is a telegram bot that I developed to help find nearby parking lots. You send the bot your current location, and it brings up the five nearest carparks from that location and lists out the carparks rate. You can also click on the carpark names returned to you and it will redirect you to google maps for you to find your current location.&lt;/p&gt;
&lt;h3&gt;bot is now down due to lack of maintenance and changes in api&lt;/h3&gt;</content:encoded></item><item><title><![CDATA[Midichlorians]]></title><description><![CDATA[A project developed for a module, Midichlorians is an iPad app implementation of the famous Midi-controller Launchpad. This project was done…]]></description><link>https://blog.tenzhiyang.com/2017-10-16-midichlorians/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-16-midichlorians/</guid><pubDate>Mon, 16 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;A project developed for a module, Midichlorians is an iPad app implementation of the famous Midi-controller Launchpad. This project was done over a period of 6 weeks, in a team of 4 people. Features include audio synch to dropbox, customised animations, recording playback, and multiple music profiles.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/43006f58db6e792f3af6cb58b0630e5a/c739e/midi.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;We won first place in our module (of 10 groups) during NUS’s
&lt;a href=&quot;http://isteps.comp.nus.edu.sg/event/10th-steps/modules&quot;&gt;10th STePS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_LfiVj5-uJ4&quot;&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/87e2b38172b1c51fbe639187da26b5fa/98431/0.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://itunes.apple.com/ca/app/midichlorians/id1229585861?mt=8&quot;&gt;Download here&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[F450 Programmable Drone]]></title><description><![CDATA[This was my first self-assembled quadcopter. Under the guidance of an NUS lecturer, Dr Colin Tan, I assembled the 450 frame drone and…]]></description><link>https://blog.tenzhiyang.com/2017-10-15-f450-drone/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-15-f450-drone/</guid><pubDate>Sun, 15 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This was my first self-assembled quadcopter. Under the guidance of an NUS lecturer, Dr Colin Tan, I assembled the 450 frame drone and changed the circuitry with 3DR’s pixhawk flight controller, and connected it to a Futaba transmitter and receiver kit.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/f7db16470fa99f5b3a89472888d6e377/c739e/f450.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Give For Free]]></title><description><![CDATA[Give For Free was a project thought up by a good friend of mine. The idea was simple: Snap pictures of your preloved goods on Give For Free…]]></description><link>https://blog.tenzhiyang.com/2017-10-14-giveforfree/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-14-giveforfree/</guid><pubDate>Sat, 14 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Give For Free was a project thought up by a good friend of mine. The idea was simple: Snap pictures of your preloved goods on Give For Free, choose your favourite charity and how much you think your items are worth. When another user wants your item, they will donate to your charity at your listed price. We managed to onboard SPCA and had over 500 users within two weeks of launch.&lt;/p&gt;
&lt;p&gt;The actual site was taken down due to server costs, and we managed to raise almost 400 dollars for SPCA. &lt;a href=&quot;https://www.facebook.com/give4free/&quot;&gt;Facebook promotional page here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/c69aa4e071f329eb83601e0673afdddf/c739e/gff.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[USB Game Controller (From Old Portfolio)]]></title><description><![CDATA[When playing Binding of Isaac which was a dual thumb stick shooter (control character with one thumb stick, control direction of your…]]></description><link>https://blog.tenzhiyang.com/2017-10-12-usb-controller/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-12-usb-controller/</guid><pubDate>Thu, 12 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;When playing Binding of Isaac which was a dual thumb stick shooter (control character with one thumb stick, control direction of your bullets with another) , I realised that the market does not really sell a controller that has dual thumb sticks in the center, most are either too far down (PS style) or at offset positions (Xbox style). As I wanted to make some custom controllers for a while (more on that when the parts arrive!), I decided to make a USB controller specifically for this game.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/2994a5cb3bff99c5b2b517068345e137/c739e/contro1.jpg&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/2e4820bef3ea9cab44f4ea7cc96450d9/c739e/contro2.jpg&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/b19253aa15dcd686a9b2ce73bcdb5f26/c739e/contro3.jpg&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/aac4692f482b300d11086c64feeb68d2/c739e/contro4.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Project Dawn (From Old Portfolio)]]></title><description><![CDATA[My first foray into Internet Of Things, this was quite a leap of faith for me. A friend (Hi Wei Lun!) asked me “is it possible to feel what…]]></description><link>https://blog.tenzhiyang.com/2017-10-11-Project-Dawn/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-11-Project-Dawn/</guid><pubDate>Wed, 11 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;My first foray into Internet Of Things, this was quite a leap of faith for me. A friend (Hi Wei Lun!) asked me “is it possible to feel what it is like to be in another persons shoes?” This would be the start of my involvement in Project Dawn.&lt;/p&gt;
&lt;p&gt;Project Dawn aimed to present a novel and innovative experiential approach to learning about the stigma surrounding mental illnesses. Essentially, Project dawn was an interactive exhibition. Powered by an Arduino Mega (clone), I had android phones connected via tcp sockets to my computer which was acting as the command center of the whole set up. The android phones would play a narration, asking participants to interact with various objects and would pause until they did the required action, after which the mobile app would continue playing the narration&lt;/p&gt;
&lt;p&gt;For Interaction with the objects, I used RFID tags and Sensors. When the object was picked up, the Sensor would stop reading signal from the RFID. The mega would then listen for when the object was placed back on again.&lt;/p&gt;
&lt;p&gt;To track which room the participants were in, I set up laser pointers wired to be permanently on, and put LDRs on the other side. This crude set up was due to my failure at implementing an Indoor Positioning System with Bluetooth, and running out of time. I could have used better forms of Proximity sensors, but waiting for delivery would have been too long. I intend to come back to IPS one day.&lt;/p&gt;
&lt;p&gt;Unfortunately I was not wise enough to take pictures of the set up, but here are a couple of pictures of the event:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/3c0c5d8e0f0704f6b3da1e74ab0eac0c/c739e/dawn1.jpg&quot;/&gt;
&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/ab7de6bec45778f99398a68f58a48470/c739e/dawn2.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[IoT Scale (From Old Portfolio)]]></title><description><![CDATA[This is a project for my internship at Boon Software for Breadtalk. Powered by an Arduino clone(again), this device uses and ESP8266 which…]]></description><link>https://blog.tenzhiyang.com/2017-10-10-iot-scale/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-10-iot-scale/</guid><pubDate>Tue, 10 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;This is a project for my internship at Boon Software for Breadtalk. Powered by an Arduino clone(again), this device uses and ESP8266 which communicates to the Arduino via Serial i/o. I wrote my own TCP Sender and Receiver on both the Arduino side as well as the Java side. There are many things that could be improved, but this is the first version.&lt;/p&gt;
&lt;p&gt;The idea of this Weighing scale is that whenever the number of bread (teabags are used in the video) falls below a certain threshold, the bakers will get a message in their kitchen that they need to bake more. This is especially useful for promotional items, such as the currently running salted egg yolk croissant where the manager is unable to use previous sales value (because it is a one-off item) to estimate how much bread each outlet needs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wi4-E1bA4Mc&quot;&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/1b0de777c1f9139a95794ec0c0de1c2f/98431/0.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[LiPo USB Charger (From Old Portfolio)]]></title><description><![CDATA[Another quick but more useful project, I used what I learnt from the bench top power supply to make this USB power-cutter (for lack of a…]]></description><link>https://blog.tenzhiyang.com/2017-10-09-drone-charger-v1/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-09-drone-charger-v1/</guid><pubDate>Mon, 09 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Another quick but more useful project, I used what I learnt from the bench top power supply to make this USB power-cutter (for lack of a better name). I needed a Device to turn off power after charging my Hubsan X4 for exactly 1 hour, as over charging the batteries was highly discouraged. I got a cheap Arduino micro clone I had lying around and attached it to a couple of female jumper wires. This was then connected to a relay. The timer is hardcoded into the Arduino. Maybe one day I will upgrade it with an LCD and some buttons, maybe use a one channel relay since that is all I need and put it in a proper enclosure. In the meantime, this will do!&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/8298575e0b879741c17850d516abac1b/c739e/lipo.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Bottle Dynamo (From Old Portfolio)]]></title><description><![CDATA[I saw this bottle dynamo on sale at Sim Lim towers and learnt that bottle dynamos could be retrofitted on most bicycles and decided to try…]]></description><link>https://blog.tenzhiyang.com/2017-10-08-Bottle-Dynamo/</link><guid isPermaLink="false">https://blog.tenzhiyang.com/2017-10-08-Bottle-Dynamo/</guid><pubDate>Sun, 08 Oct 2017 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;I saw this bottle dynamo on sale at Sim Lim towers and learnt that bottle dynamos could be retrofitted on most bicycles and decided to try to add it on my bike. I learnt to make a diode bridge to change the AC circuit into DC, and put a capacitor over the circuit so that the light would not blink. Pretty simple circuit, and the results were decent although not very pretty.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;gatsby-remark-images-extra&quot; src=&quot;/static/8adb2ad86bcbfdd71ed69df8e345517f/c739e/bike.jpg&quot;/&gt;&lt;/p&gt;</content:encoded></item></channel></rss>